{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wGMyKMCDzRfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --------------------------------------- 0.0/12.8 MB 262.6 kB/s eta 0:00:49\n",
            "     --------------------------------------- 0.1/12.8 MB 819.2 kB/s eta 0:00:16\n",
            "     - -------------------------------------- 0.4/12.8 MB 2.4 MB/s eta 0:00:06\n",
            "     -- ------------------------------------- 0.9/12.8 MB 3.9 MB/s eta 0:00:04\n",
            "     ---- ----------------------------------- 1.4/12.8 MB 5.2 MB/s eta 0:00:03\n",
            "     ------ --------------------------------- 1.9/12.8 MB 6.2 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 2.5/12.8 MB 6.9 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.0/12.8 MB 7.4 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.6/12.8 MB 7.9 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.2/12.8 MB 8.3 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.7/12.8 MB 8.6 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 5.3/12.8 MB 8.8 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 5.8/12.8 MB 9.1 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 6.2/12.8 MB 9.2 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 6.9/12.8 MB 9.4 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 7.5/12.8 MB 9.5 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 8.0/12.8 MB 9.6 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.5/12.8 MB 9.7 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 9.1/12.8 MB 9.9 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 9.7/12.8 MB 10.0 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 10.2/12.8 MB 10.0 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 10.8/12.8 MB 11.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 11.3/12.8 MB 11.9 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 11.9/12.8 MB 11.9 MB/s eta 0:00:01\n",
            "     ------------------------------------- - 12.4/12.8 MB 11.9 MB/s eta 0:00:01\n",
            "     --------------------------------------  12.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
            "     --------------------------------------  12.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 10.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\huynhlbg\\documents\\github\\rat-sql-experiment\\.venv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "%pip install gdown spacy --quiet\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kXJ5VJX25HYq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import torch\n",
        "# Prepare Stanford's Stanza to tokenize the columns and tables names\n",
        "# stanza.download('en')\n",
        "# stanza_nlp = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='en')\n",
        "\n",
        "# Prepare spacy to tokenize and lemmatize the table names and columns names\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\", \"parser\"])\n",
        "# Fix the \"Spacy breaks up 'id' into 'i' and 'd' issue\" https://github.com/explosion/spaCy/discussions/10570\n",
        "spacy_nlp.tokenizer.rules = { key: value for key, value in spacy_nlp.tokenizer.rules.items() if key != \"id\" }\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "  device = \"mps\" # Apple Metal Performance Shader (M1 chip)\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "print(f\"Using {device} device\")\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VKMceBe22STs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "spider_url = 'https://drive.google.com/u/0/uc?id=1iRDVHLr4mX2wQKSgA9J8Pire73Jahh0m&export=download' # https://yale-lily.github.io/spider\n",
        "output = 'spider.zip'\n",
        "if not os.path.exists(output):\n",
        "  gdown.download(spider_url, output, quiet=False)\n",
        "\n",
        "!if [ ! -d \"spider\" ]; then unzip spider.zip -d .; else echo \"The 'spider' folder already exists.\"; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz75rWXgzBRd",
        "outputId": "280a2d12-e04a-4388-906a-aa9655690682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column(db_id='perpetrator', table_index=0, col_index=1, name='perpetrator id', name_toks=['perpetrator', 'id'], orig_name='Perpetrator_ID', col_type='number')\n",
            "Database(db_id='entrepreneur', raw_schema={'column_names': [[-1, '*'], [0, 'entrepreneur id'], [0, 'people id'], [0, 'company'], [0, 'money requested'], [0, 'investor'], [1, 'people id'], [1, 'name'], [1, 'height'], [1, 'weight'], [1, 'date of birth']], 'column_names_original': [[-1, '*'], [0, 'Entrepreneur_ID'], [0, 'People_ID'], [0, 'Company'], [0, 'Money_Requested'], [0, 'Investor'], [1, 'People_ID'], [1, 'Name'], [1, 'Height'], [1, 'Weight'], [1, 'Date_of_Birth']], 'column_types': ['text', 'number', 'number', 'text', 'number', 'text', 'number', 'text', 'number', 'number', 'text'], 'db_id': 'entrepreneur', 'foreign_keys': [[2, 6]], 'primary_keys': [1, 6], 'table_names': ['entrepreneur', 'people'], 'table_names_original': ['entrepreneur', 'people']}, tables=[Table(db_id='entrepreneur', table_index=0, name='entrepreneur', name_toks=['entrepreneur'], orig_name='entrepreneur', columns=[Column(db_id='entrepreneur', table_index=0, col_index=1, name='entrepreneur id', name_toks=['entrepreneur', 'id'], orig_name='Entrepreneur_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=2, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=3, name='company', name_toks=['company'], orig_name='Company', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=4, name='money requested', name_toks=['money', 'request'], orig_name='Money_Requested', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=5, name='investor', name_toks=['investor'], orig_name='Investor', col_type='text')]), Table(db_id='entrepreneur', table_index=1, name='people', name_toks=['people'], orig_name='people', columns=[Column(db_id='entrepreneur', table_index=1, col_index=6, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=7, name='name', name_toks=['name'], orig_name='Name', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=8, name='height', name_toks=['height'], orig_name='Height', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=9, name='weight', name_toks=['weight'], orig_name='Weight', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=10, name='date of birth', name_toks=['date', 'of', 'birth'], orig_name='Date_of_Birth', col_type='text')])], columns=[Column(db_id='entrepreneur', table_index=-1, col_index=0, name='*', name_toks=['*'], orig_name='*', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=1, name='entrepreneur id', name_toks=['entrepreneur', 'id'], orig_name='Entrepreneur_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=2, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=3, name='company', name_toks=['company'], orig_name='Company', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=4, name='money requested', name_toks=['money', 'request'], orig_name='Money_Requested', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=5, name='investor', name_toks=['investor'], orig_name='Investor', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=6, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=7, name='name', name_toks=['name'], orig_name='Name', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=8, name='height', name_toks=['height'], orig_name='Height', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=9, name='weight', name_toks=['weight'], orig_name='Weight', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=10, name='date of birth', name_toks=['date', 'of', 'birth'], orig_name='Date_of_Birth', col_type='text')])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, Any, Union\n",
        "\n",
        "# Start by loading in the databases, tables, and columns schemas\n",
        "\n",
        "with open(\"spider/tables.json\") as f:\n",
        "  raw_schema_data = json.load(f)\n",
        "\n",
        "ColumnType = Literal[\"time\", \"text\", \"number\", \"boolean\", \"others\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Column:\n",
        "  db_id: str  # Which database this col belongs to\n",
        "  table_index: int  # Which specific table in the db this col belongs to\n",
        "  col_index: int  # Which specific column in this table\n",
        "  name: str\n",
        "  name_toks: list[str]  # Tokenized name\n",
        "  orig_name: str\n",
        "  col_type: ColumnType\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Table:\n",
        "  db_id: str  # Which database this table belongs to\n",
        "  table_index: int  # Which specific table in this database\n",
        "  name: str\n",
        "  name_toks: list[str]  # Tokenized name\n",
        "  orig_name: str\n",
        "  columns: list[Column] = field(default_factory=list)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Database:\n",
        "  db_id: str\n",
        "  raw_schema: Any  # The original raw schema from json file\n",
        "  tables: list[Table] = field(default_factory=list)\n",
        "  columns: list[Column] = field(default_factory=list)\n",
        "\n",
        "\n",
        "schema_lookup: dict[str, Database] = {}  # Map the db_id to that database's schema\n",
        "\n",
        "raw_schema_data[0].keys()\n",
        "# ['column_names', 'column_names_original', 'column_types', 'db_id',\n",
        "# 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original']\n",
        "\n",
        "\n",
        "def lemmatized_tokenizer(s: str):\n",
        "  doc = spacy_nlp(s)\n",
        "  lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
        "  # doc = stanza_nlp(s)\n",
        "  # lemmatized_tokens = [word.lemma.lower() for sent in doc.sentences  for word in sent.words]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "\n",
        "for i, db_info in enumerate(raw_schema_data):\n",
        "  db_id = db_info[\"db_id\"]\n",
        "  db = Database(db_id, raw_schema=db_info)\n",
        "\n",
        "  # Iterate the tables of this database\n",
        "  for table_index, (tbl_name, tbl_orig_name) in enumerate(\n",
        "    zip(db_info[\"table_names\"], db_info[\"table_names_original\"])\n",
        "  ):\n",
        "    new_table = Table(\n",
        "      db_id,\n",
        "      table_index=table_index,\n",
        "      name=tbl_name,\n",
        "      name_toks=lemmatized_tokenizer(tbl_name),\n",
        "      orig_name=tbl_orig_name,\n",
        "    )\n",
        "    db.tables.append(new_table)\n",
        "\n",
        "  # Iterate the columns of this database\n",
        "  for col_index, ((table_idx, col_name), (_, col_orig_name), col_type) in enumerate(\n",
        "    zip(\n",
        "        db_info[\"column_names\"],\n",
        "        db_info[\"column_names_original\"],\n",
        "        db_info[\"column_types\"],\n",
        "    )\n",
        "  ):\n",
        "    new_col = Column(\n",
        "        db_id=db_id,\n",
        "        col_index=col_index,\n",
        "        col_type=col_type,\n",
        "        table_index=table_idx,\n",
        "        name=col_name,\n",
        "        name_toks=lemmatized_tokenizer(col_name),\n",
        "        orig_name=col_orig_name,\n",
        "    )\n",
        "    db.columns.append(new_col)\n",
        "    table_idx: int = table_idx\n",
        "    if table_idx >= 0:  # Skip the -1 index table\n",
        "        db.tables[table_idx].columns.append(new_col)\n",
        "\n",
        "  schema_lookup[db_id] = db\n",
        "\n",
        "all_columns = [col for s in schema_lookup.values() for col in s.columns]\n",
        "all_tables = [tbl for s in schema_lookup.values() for tbl in s.tables]\n",
        "\n",
        "print(all_columns[1])\n",
        "print(schema_lookup[\"entrepreneur\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "wv5MghOvMVqh"
      },
      "outputs": [],
      "source": [
        "# Load the questions\n",
        "with open(\"spider/train_spider.json\") as f:\n",
        "  raw_train_data = json.load(f)\n",
        "\n",
        "with open(\"spider/dev.json\") as f:\n",
        "  raw_val_data = json.load(f)\n",
        "\n",
        "raw_train_data[0].keys()\n",
        "# ['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql']\n",
        "\n",
        "@dataclass\n",
        "class QAPair: # Class to encapsulate each NL query with corresponding SQL answer\n",
        "  db_id: str\n",
        "  question_toks: list[str]\n",
        "  question: str\n",
        "  query_toks: list[str]\n",
        "  query: str\n",
        "  sql_tree: Any # Raw sql tree as a nested dict from the raw JSON\n",
        "\n",
        "@dataclass\n",
        "class SpiderItem:\n",
        "  \"\"\"An item to feed into the neural network. Basically encapsulation of a training example\"\"\"\n",
        "  db_id: str\n",
        "  qa_pair: QAPair\n",
        "  db: Database # Information about the database used in the qa\n",
        "\n",
        "\n",
        "train_qas: list[QAPair] = []\n",
        "val_qas: list[QAPair] = []\n",
        "train_items: list[SpiderItem] = []\n",
        "val_items: list[SpiderItem] = []\n",
        "\n",
        "for i, qa_info in enumerate(raw_train_data):\n",
        "  db_id = qa_info[\"db_id\"]\n",
        "  new_qa = QAPair(\n",
        "    db_id=db_id,\n",
        "    question_toks=qa_info[\"question_toks\"],\n",
        "    question=qa_info[\"question\"],\n",
        "    query_toks=qa_info[\"query_toks\"],\n",
        "    query=qa_info[\"query\"],\n",
        "    sql_tree=qa_info[\"sql\"]\n",
        "  )\n",
        "  new_item = SpiderItem(\n",
        "    db_id=db_id,\n",
        "    qa_pair=new_qa,\n",
        "    db=schema_lookup[db_id]\n",
        "  )\n",
        "  train_qas.append(new_qa)\n",
        "  train_items.append(new_item)\n",
        "\n",
        "for i, qa_info in enumerate(raw_val_data):\n",
        "  db_id = qa_info[\"db_id\"]\n",
        "  new_qa = QAPair(\n",
        "    db_id=db_id,\n",
        "    question_toks=qa_info[\"question_toks\"],\n",
        "    question=qa_info[\"question\"],\n",
        "    query_toks=qa_info[\"query_toks\"],\n",
        "    query=qa_info[\"query\"],\n",
        "    sql_tree=qa_info[\"sql\"]\n",
        "  )\n",
        "  new_item = SpiderItem(\n",
        "    db_id=db_id,\n",
        "    qa_pair=new_qa,\n",
        "    db=schema_lookup[db_id]\n",
        "  )\n",
        "  val_qas.append(new_qa)\n",
        "  val_items.append(new_item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK7Sg2WoIT2C",
        "outputId": "556adaf2-cd93-43ac-9057-d154b9da4943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'number', 'others', 'boolean', 'time', 'text'}\n",
            "Vocab size col names 1176\n",
            "Vocab size tbl names 506\n",
            "['connection', 'industry', 'apartment', 'sf', 'bfp', 'ghome', 'username', 'cattle', 'music', 'amerindian']\n",
            "['gas', 'apartment', 'broadcast', 'pit', 'music', 'settlement', 'grape', 'call', 'copy', 'actual']\n"
          ]
        }
      ],
      "source": [
        "col_name_vocab: set[str] = set()\n",
        "tbl_name_vocab: set[str] = set()\n",
        "col_types: set[str] = set()\n",
        "\n",
        "# Build col names and tbl names vocab\n",
        "for col in all_columns:\n",
        "  col_name_vocab.update(col.name_toks)\n",
        "  col_types.add(col.col_type)\n",
        "\n",
        "for tbl in all_tables:\n",
        "  tbl_name_vocab.update(tbl.name_toks)\n",
        "\n",
        "print(col_types) # {'text', 'others', 'number', 'time', 'boolean'}\n",
        "print(\"Vocab size col names\", len(col_name_vocab))\n",
        "print(\"Vocab size tbl names\", len(tbl_name_vocab))\n",
        "print(list(col_name_vocab)[0:10])\n",
        "print(list(tbl_name_vocab)[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFKZz5rdySzh",
        "outputId": "0c14015e-1c57-4854-f94c-5af7431a1673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size questions 3750\n",
            "Vocab size SQL 5468\n"
          ]
        }
      ],
      "source": [
        "target_vocab: set[str] = set() # Vocab of the SQL target\n",
        "question_vocab: set[str] = set() # Vocab of the Natural Language questions\n",
        "\n",
        "for qa_pair in train_qas:\n",
        "  question_vocab.update(qa_pair.question_toks)\n",
        "  target_vocab.update(qa_pair.query_toks)\n",
        "\n",
        "for qa_pair in val_qas:\n",
        "  question_vocab.update(qa_pair.question_toks)\n",
        "  target_vocab.update(qa_pair.query_toks)\n",
        "\n",
        "# Padding token used to pad shorter inputs so every sequence of inputs to the encoder\n",
        "# is of same length (which would be the max length)\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "src_vocab: set[str] = set() # Source vocab used for inputs to the encoder, which includes questions & columns & tables names\n",
        "src_vocab.add(PAD_TOKEN)\n",
        "# Add the \"CLS\" token as a separator between each part in the input block (i.e. separator between each column name, table name, etc.)\n",
        "src_vocab.add(\"CLS\")\n",
        "src_vocab.update(col_name_vocab)\n",
        "src_vocab.update(col_types)\n",
        "src_vocab.update(tbl_name_vocab)\n",
        "src_vocab.update(question_vocab)\n",
        "\n",
        "tgt_vocab: set[str] = set() # Target vocab used for decoder output; which is the set of SQL tokens plus the Start and End tokens\n",
        "tgt_vocab.add(\"<START>\")\n",
        "tgt_vocab.add(\"<END>\")\n",
        "tgt_vocab.add(PAD_TOKEN)\n",
        "tgt_vocab.update(target_vocab)\n",
        "\n",
        "\n",
        "print(\"Vocab size questions\", len(question_vocab))\n",
        "print(\"Vocab size SQL\", len(target_vocab))\n",
        "\n",
        "# Token to id: Assign a number to each token in the vocab\n",
        "# Id to token: Retrieve the token give the index\n",
        "src_tok_to_idx = { tok: i for i, tok in enumerate(src_vocab) }\n",
        "src_idx_to_tok = { i: tok for i, tok in enumerate(src_vocab) }\n",
        "tgt_tok_to_idx = { tok: i for i, tok in enumerate(tgt_vocab) }\n",
        "tgt_idx_to_tok = { i: tok for i, tok in enumerate(tgt_vocab) }\n",
        "\n",
        "def toks_encode(tokens_list: list[str], mode: Literal[\"source\", \"target\"]):\n",
        "  \"\"\"Given a list of tokens, 'encode' them to return a list of token_ids\n",
        "  Parameters:\n",
        "    mode: Whether to use the source's or target's tok_to_idx lookup table\n",
        "  \"\"\"\n",
        "  if mode == \"source\":\n",
        "    return [src_tok_to_idx[token] for token in tokens_list]\n",
        "  return [tgt_tok_to_idx[token] for token in tokens_list]\n",
        "\n",
        "def toks_decode(ids_list: list[int], mode: Literal[\"source\", \"target\"]):\n",
        "  \"\"\"Given a list of token_ids, 'decode' them to return the list of corresponding token\n",
        "  Parameters:\n",
        "    mode: Whether to use the source's or target's idx_to_tok lookup table\n",
        "  \"\"\"\n",
        "  if mode == \"source\":\n",
        "    return [src_idx_to_tok[token_id] for token_id in ids_list]\n",
        "  return [tgt_idx_to_tok[token_id] for token_id in ids_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzijLcv7E5F8",
        "outputId": "7e915550-7778-497d-f91d-0b3ff2878aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7000\n",
            "1034\n",
            "[1196, 70, 44, 91]\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset and dataloader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def construct_input_block_toks(\n",
        "    col_toks: list[list[str]],\n",
        "    col_types: list[ColumnType],\n",
        "    table_toks: list[list[str]],\n",
        "    ques_toks: list[str],\n",
        "):\n",
        "    \"\"\"Construct and return the 3 components of an input block as a tuple of the list of tokens\n",
        "    An input block has 3 parts:\n",
        "      The col name part: `CLS col_type ...col_name CLS col_type2 ...Col_name2`\n",
        "      The table name part: `CLS ...tbl_name CLS ...tbl_name2`\n",
        "      The question part: ` CLS ...question`\n",
        "\n",
        "    Parameters:\n",
        "      col_toks: nested list of all tokens across all column names, each element is a list of tokens that belongs in one column name\n",
        "      tbl_toks: nested list of all tokens across all table names, each element is a list of tokens that belongs in one table name\n",
        "\n",
        "    Returns:\n",
        "      a 3-elemment tuple containing (col_name_part, tbl_name_part, ques_part) w/ each part being the list of tokens including the 'CLS' token\n",
        "    \"\"\"\n",
        "    # List of column types should match length with list of columns\n",
        "    assert len(col_toks) == len(col_types)\n",
        "    # Construct the col name part, thus producing something like:\n",
        "    # \"CLS number builing size CLS text building name...\"\n",
        "    col_name_part: list[str] = []\n",
        "    for col_name_toks, col_type in zip(col_toks, col_types):\n",
        "        col_name_part.extend([\"CLS\", col_type] + col_name_toks)\n",
        "    # Construct the col name part, thus producing something like:\n",
        "    # \"CLS building information CLS city information ...etc\"\n",
        "    tbl_name_part: list[str] = []\n",
        "    for tbl_name_toks in table_toks:\n",
        "        tbl_name_part.extend([\"CLS\"] + tbl_name_toks)\n",
        "    # Construct the question part, thus producing something like:\n",
        "    # \"CLS what is the highest building in Chicago\"\n",
        "    question_part: list[str] = ques_toks\n",
        "    return (col_name_part, tbl_name_part, question_part)\n",
        "\n",
        "DatasetItem = tuple[list[str], list[str], list[str], list[str]]\n",
        "class MyDataset(Dataset[DatasetItem]):\n",
        "    \"\"\"Each element in this dataset is a 4-element tuple\n",
        "    (col_name_toks, table_name_toks, question_toks, target_toks)\n",
        "\n",
        "    where `col_name_toks`, `table_name_toks`, `question_toks` has been preprocessed to add \"CLS\" separator token\n",
        "    and `target_toks` is also processed to include <Start> and <End> tokens\n",
        "    \"\"\"\n",
        "    def __init__(self, all_items: list[SpiderItem]):\n",
        "        self.items = all_items\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> DatasetItem:\n",
        "        item = self.items[idx]\n",
        "        columns_tokens = [c.name_toks for c in item.db.columns[1:]] # Skip the 0th-index \"*\" column\n",
        "        columns_types: list[ColumnType] = [c.col_type for c in item.db.columns[1:]]\n",
        "        tables_tokens = [t.name_toks for t in item.db.tables]\n",
        "        question_tokens = item.qa_pair.question_toks\n",
        "        target_tokens = item.qa_pair.query_toks + [\"<END>\"]\n",
        "\n",
        "        return construct_input_block_toks(columns_tokens, columns_types, tables_tokens, question_tokens) + (target_tokens,)\n",
        "\n",
        "\n",
        "train_dataset = MyDataset(train_items)\n",
        "val_dataset = MyDataset(val_items)\n",
        "print(len(train_dataset))  # Should be 7000\n",
        "print(len(val_dataset)) # Should be 1034\n",
        "\n",
        "PAD_TOKEN_ID = src_tok_to_idx[PAD_TOKEN]\n",
        "\n",
        "# Find the max_length of each \"category\".\n",
        "# Category here correspondings to the 4 categories (col_name_toks, table_name_toks, question_toks, target_toks)\n",
        "max_lengths = [max(len(category) for category in item) for item in zip(*train_dataset)]\n",
        "print(max_lengths) # [1196, 70, 44, 92]\n",
        "max_len_col_name_part = max_lengths[0]\n",
        "max_len_tbl_name_part = max_lengths[1]\n",
        "max_len_ques_part = max_lengths[2]\n",
        "max_len_target = max_lengths[3]\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "ModelInput = tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "\n",
        "def pad_collate(batch: list[tuple[list[str], list[str], list[str], list[str]]]) -> ModelInput:\n",
        "    \"\"\"The collate_fn to pass to Pytorch Dataloader.\n",
        "    Pad each element in the dataset so they all have the same size\"\"\"\n",
        "    batched_col_name_toks, batched_tbl_name_toks, batched_ques_toks, batched_target_toks = zip(*batch)\n",
        "\n",
        "    batched_colname_tokIds = [torch.tensor(toks_encode(col_name_toks, mode=\"source\")) for col_name_toks in batched_col_name_toks]\n",
        "    # Now pad this batch so that every item of the batch is same size as a local longest item\n",
        "    # From the doc: `pad_sequence` stacks a list of Tensors along a new dimension, and pads them to equal length.\n",
        "    batched_colname_tokIds = pad_sequence(batched_colname_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    # Now pad this batch again so that every item of the batch is same size of the global longest item (max_len_col_name_part)\n",
        "    batched_colname_tokIds= torch.nn.functional.pad(\n",
        "        batched_colname_tokIds, (0, max_len_col_name_part - batched_colname_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_colname_tokIds = batched_colname_tokIds.to(DEVICE) # (B, T)\n",
        "\n",
        "    # After padding, each tensor should be dimension (B, T)\n",
        "    # With B being the batch dimension (since we use batch_first=True) and T is max_len_col_name_part\n",
        "    assert batched_colname_tokIds.shape == (BATCH_SIZE, max_len_col_name_part), f\"Expected {BATCH_SIZE, max_len_col_name_part}, got {batched_colname_tokIds.shape}\"\n",
        "\n",
        "    # Do the same process for table name toks, question toks, and target toks\n",
        "    batched_tblname_tokIds = [torch.tensor(toks_encode(tbl_name_toks, mode=\"source\")) for tbl_name_toks in batched_tbl_name_toks]\n",
        "    batched_tblname_tokIds = pad_sequence(batched_tblname_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    batched_tblname_tokIds= torch.nn.functional.pad(\n",
        "        batched_tblname_tokIds, (0, max_len_tbl_name_part - batched_tblname_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_tblname_tokIds = batched_tblname_tokIds.to(DEVICE)\n",
        "    assert batched_tblname_tokIds.shape == (BATCH_SIZE, max_len_tbl_name_part)\n",
        "\n",
        "    batched_ques_tokIds = [torch.tensor(toks_encode(ques_toks, mode=\"source\")) for ques_toks in batched_ques_toks]\n",
        "    batched_ques_tokIds = pad_sequence(batched_ques_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    batched_ques_tokIds = torch.nn.functional.pad(\n",
        "        batched_ques_tokIds, (0, max_len_ques_part - batched_ques_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_ques_tokIds = batched_ques_tokIds.to(DEVICE)\n",
        "    assert batched_ques_tokIds.shape == (BATCH_SIZE, max_len_ques_part)\n",
        "\n",
        "    batched_tgt_tokIds = [torch.tensor(toks_encode(tgt_toks, mode=\"target\")) for tgt_toks in batched_target_toks]\n",
        "    # Note that for the target, we need use `tgt_tok_to_idx`\n",
        "    batched_tgt_tokIds = pad_sequence(batched_tgt_tokIds, batch_first=True, padding_value=tgt_tok_to_idx[PAD_TOKEN])\n",
        "    batched_tgt_tokIds = torch.nn.functional.pad(\n",
        "        batched_tgt_tokIds, (0, max_len_target - batched_tgt_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_tgt_tokIds = batched_tgt_tokIds.to(DEVICE)\n",
        "    assert batched_tgt_tokIds.shape == (BATCH_SIZE, max_len_target)\n",
        "\n",
        "    return (batched_colname_tokIds, batched_tblname_tokIds, batched_ques_tokIds, batched_tgt_tokIds)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzuU6VkB3tuw"
      },
      "source": [
        "# Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C46jlVZY4H8A"
      },
      "source": [
        "## The Masked Attention Math trick\n",
        "\n",
        "The Triangular mask technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7PwwGDegHS2",
        "outputId": "9be7ecec-bcfc-42fa-ee7a-25933c2e0d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# The Attention Mask Math trick - The Triangular mask technique\n",
        "T = 300\n",
        "\n",
        "# A triangular mask is a tensor like this: dimension (block_size, block_size)\n",
        "# tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
        "#        [1., 1., 0.,  ..., 0., 0., 0.],\n",
        "#        [1., 1., 1.,  ..., 0., 0., 0.],\n",
        "#        ...,\n",
        "#        [1., 1., 1.,  ..., 1., 0., 0.],\n",
        "#        [1., 1., 1.,  ..., 1., 1., 0.],\n",
        "#        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
        "tril_mask = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# Initial weight represents the initial *affinity* between each pair of tokens in the sequence\n",
        "# (right now we initialize them all with 0s, but they can be diff numbers coming from somewhere like the encoder for example)\n",
        "initial_weight = torch.zeros((T, T))\n",
        "\n",
        "# For the purpose of decoder calculation, we want prediction of the next step to only depend on values of previous steps\n",
        "# Therefore, we'll apply a *triangular mask* to the weight vector\n",
        "# By the structure of the triangular mask, we can see that, after masking,\n",
        "# we essentially \"disable\"/\"mask away\" future tokens since they will be multiplying with 0\n",
        "# since only positions corresponding to 1s will be retained. Positions with 0s is replaced with -inf\n",
        "weight = initial_weight.masked_fill(tril_mask == 0, float(\"-inf\"))\n",
        "print(weight)\n",
        "# A softmax layer convert these *affinity scores* to a weight number between 0 and 1\n",
        "weight = F.softmax(weight, dim=-1)\n",
        "print(weight)\n",
        "# tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        ...,\n",
        "#        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
        "#        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
        "#        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n",
        "# --> See how the weight vector showcase how to calculate a prediction based on the weighted sum of previous values only\n",
        "# For example:\n",
        "#   2nd value = 1.00 * 1st val\n",
        "#   3rd prediction = 0.5 * 1st val + 0.5 * 2nd val\n",
        "#   4th prediction = 0.33 * 1st value + 0.33 * 2nd val + 0.33 * 3rd val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modified Tensor:\n",
            "tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Example tensor and list of indexes\n",
        "tensor = torch.arange(15)  # Example tensor of size 15\n",
        "indexes = [1, 4, 9, 11]  # Example list of indexes to zero out\n",
        "\n",
        "# Create a tensor of ones with the same size as the original tensor\n",
        "modified_tensor = torch.ones_like(tensor)\n",
        "\n",
        "# Set the elements at the specified indexes to 0\n",
        "modified_tensor[indexes] = 0\n",
        "\n",
        "print(\"Modified Tensor:\")\n",
        "print(modified_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNeqGOj4RSw"
      },
      "source": [
        "## Self-attention block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Un6Q6p044kTN"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "BLOCK_SIZE = max_len_col_name_part + max_len_tbl_name_part + max_len_ques_part\n",
        "DROP_OUT = 0.2\n",
        "\n",
        "class SingleHeadAttention(nn.Module):\n",
        "  def __init__(self, emb_size: int, head_size: int):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.head_size = head_size\n",
        "    # Each head has its own W_Q, W_K, and W_V matrixes for transform the each tok emd to its corresponding q, k, v vectors\n",
        "    self.query_matrix = nn.Linear(emb_size, head_size, bias=False)\n",
        "    self.key_matrix = nn.Linear(emb_size, head_size, bias=False)\n",
        "    self.value_matrix = nn.Linear(emb_size, head_size, bias=False)\n",
        "    # tril_mask is a static non-learned parameter, so need to use `register_buffer`\n",
        "    self.register_buffer(\"tril_mask\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "    self.dropout = nn.Dropout(DROP_OUT)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, should_mask:bool):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      x: the input embedding (after summing token_emb and position_emb) --> dimension (B, T, E)\n",
        "      should_mask: should this Attention block use masked attention (decoder should use mask, encoder shouldn't)\n",
        "    \"\"\"\n",
        "    query = self.query_matrix(x) # (B, T, D) with D = head_size\n",
        "    key = self.key_matrix(x) # (B, T, D)\n",
        "    value = self.value_matrix(x) # (B, T, D)\n",
        "    # `q @ k.T` will produce the itcanitial affinity matrix, basically how strong each query relates to each key\n",
        "    # dimension: (B, T, D) @ (B, D, T) = (B, T, T)\n",
        "    # Note The original \"Attention is all you need paper\" also scales down the affinity scores by multiplying `sqrt(head_size)`\n",
        "    affinity = (query @ key.transpose(-2, -1)) * (sqrt(self.head_size))  # tranpose(-2, -1) avoid transposing the Batch dimension\n",
        "    if should_mask:\n",
        "      affinity = affinity.masked_fill(self.tril_mask == 0, float(\"-inf\"))\n",
        "    weight = F.softmax(affinity, dim=-1) # (B, T, T)\n",
        "    weight = self.dropout(weight)\n",
        "    # The output is the embeddings where each token's embedding have been tweaked\n",
        "    # to also include information about other related tokens\n",
        "    out = weight @ value # (B, T, D)\n",
        "    return out\n",
        "  \n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, emb_size: int, num_head: int):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    # Each head size is emb_size / num_head so that at the end, when we concat all vectors from each head, we still get a vector of emb_size\n",
        "    self.heads = nn.ModuleList([SingleHeadAttention(emb_size, emb_size // num_head) for _ in range(num_head)])\n",
        "    self.dropout = nn.Dropout(DROP_OUT)\n",
        "  \n",
        "  def forward(self, x: torch.Tensor, should_mask: bool=False):\n",
        "    out = torch.cat([sa(x, should_mask=should_mask) for sa in self.heads], dim=-1)\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "  \n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "  \"\"\"After self-attention block is a Feed forward neural net (section 3.3)\"\"\"\n",
        "  def __init__(self, emb_size: int):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "      nn.Linear(emb_size, 4 * emb_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * emb_size, emb_size),\n",
        "      nn.Dropout(DROP_OUT)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.feed_forward(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  \"\"\"A Transformer Encoder block: A self-attention followed by feedforward net\"\"\"\n",
        "  def __init__(self, emb_size: int, num_attention_heads: int):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHeadAttention(emb_size, num_attention_heads)\n",
        "    self.feed_forward = PositionWiseFeedForward(emb_size)\n",
        "    self.layer_norm1 = nn.LayerNorm(emb_size) # Layer norm for the self-attention sublayer\n",
        "    self.layer_norm2 = nn.LayerNorm(emb_size) # Layer norm for the feed-forward sublayer\n",
        "  \n",
        "  def forward(self, x: torch.Tensor):\n",
        "    # Addition is needed to represent additive residual connection\n",
        "    x = x + self.self_attention(self.layer_norm1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Positional Encoding\n",
        "Use the sine and cosine positional encoding scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00],\n",
            "        [ 8.4147e-01,  5.4030e-01,  8.0196e-01,  ...,  1.0000e+00,\n",
            "          1.0746e-04,  1.0000e+00],\n",
            "        [ 9.0930e-01, -4.1615e-01,  9.5814e-01,  ...,  1.0000e+00,\n",
            "          2.1492e-04,  1.0000e+00],\n",
            "        ...,\n",
            "        [ 9.7302e-02,  9.9525e-01, -4.4514e-01,  ...,  9.8863e-01,\n",
            "          1.3999e-01,  9.9015e-01],\n",
            "        [ 8.9005e-01,  4.5586e-01, -9.8404e-01,  ...,  9.8861e-01,\n",
            "          1.4010e-01,  9.9014e-01],\n",
            "        [ 8.6449e-01, -5.0265e-01, -7.3058e-01,  ...,  9.8860e-01,\n",
            "          1.4020e-01,  9.9012e-01]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Credit: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "# Adjusted so that input `x` can be of shape `[batch_size, seq_len, embedding_dim]`\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def compute_pos_encoding(block_size: int, d_model: int):\n",
        "  positions = torch.arange(block_size).unsqueeze(1)\n",
        "  div_terms = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "  position_encodings = torch.zeros(block_size, d_model, device=DEVICE) # (B, T, E)\n",
        "  position_encodings[:, 0::2] = torch.sin(positions * div_terms)\n",
        "  position_encodings[:, 1::2] = torch.cos(positions * div_terms)\n",
        "  return position_encodings\n",
        "\n",
        "pe = compute_pos_encoding(BLOCK_SIZE, 256)\n",
        "print(pe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JumzVuyT4X-f"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "quKt89aqF6zC"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "class Transformer1(nn.Module):\n",
        "  def __init__(self, emb_size: int = 256):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      emb_size: the size of each word embeddings. For example: GloVe embeddings is 300, BERT is 768\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    # 4 encoder blocks\n",
        "    self.encoder_layers = nn.Sequential(\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      nn.LayerNorm(emb_size)\n",
        "    )\n",
        "    # ENCODER COMPONENTS\n",
        "    self.encoder_token_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "    # Position embedding table: Convert each token's position in its block to a position embedding\n",
        "    # Since it is using the sine and cosine positional encoding scheme, it's actually static\n",
        "    self.register_buffer(\"positional_embedding\", compute_pos_encoding(BLOCK_SIZE, emb_size))\n",
        "\n",
        "    # DECODER COMPONENTS\n",
        "    self.decoder_hidden_state_size = 2 * emb_size\n",
        "    # Embedding lookup table: Convert token_ids to that token's corresponding embeddings\n",
        "    self.decoder_token_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
        "    # Target language modeling head: Transform back from the embedding dimension to the tgt_vocab_size dimension\n",
        "    # So that we can get the distribution and know which target token to choose\n",
        "    self.tgt_lm_head = nn.Linear(self.decoder_hidden_state_size, tgt_vocab_size)\n",
        "    # For the decoder, we try to replicate the RNN model to process sequences\n",
        "    # decoder_hidden_state = sigmoid(W1 * context_matrix + W2 * prev_hidden_state + bias + W3 * decoder_input_tok_emb)\n",
        "    # Thus we need 3 weights matrix for use in the decoder, to produce a new decoder_hidden_state\n",
        "    self.decoder_context_linear = nn.Linear(emb_size, self.decoder_hidden_state_size)\n",
        "    self.decoder_hiddenstate_linear = nn.Linear(self.decoder_hidden_state_size, self.decoder_hidden_state_size)\n",
        "    self.decoder_token_emb_linear = nn.Linear(emb_size, self.decoder_hidden_state_size)\n",
        "\n",
        "  def forward(self, input_idx: torch.Tensor, target_idx: Union[torch.Tensor, None] = None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      target_idx: the list of target tokens across the batches. Dimension (B, T)\n",
        "    \"\"\"\n",
        "    x = self.encode(input_idx) # (B, T, E)\n",
        "    # Average the last hidden state of the encoder as the context\n",
        "    context_emb = torch.mean(x, dim=1) # (B, E)\n",
        "    # Feed the <START> token as the chosen token to the entire batch\n",
        "    chosen_tokens = torch.tensor(toks_encode([\"<START>\"] * x.size(0), \"target\"), device=DEVICE) # (B)\n",
        "    ys: list[torch.Tensor] = []\n",
        "    # Initialize the first hidden state as 0s\n",
        "    hidden_state = torch.zeros((BATCH_SIZE, self.decoder_hidden_state_size), device=DEVICE) # (B, H) where H = dec_hidden_state\n",
        "    for _ in range(max_len_target):\n",
        "      # hidden_state: (B, H)\n",
        "      # tgt_probs: (B, C) where C = tgt_vocab_size\n",
        "      hidden_state, tgt_probs = self.decode(context_emb, chosen_tokens, hidden_state)\n",
        "      # Greedily select the token with highest prob from the distribution\n",
        "      chosen_tokens = torch.argmax(tgt_probs, dim=1) # (B)\n",
        "      ys.append(tgt_probs)\n",
        "\n",
        "    # Note that ys is collected by looping over max_len_target, so when stacked, the first dimension is max_len_target\n",
        "    y = torch.stack(ys) # (T, B, C) where C = tgt_vocab_size\n",
        "    assert y.shape == (max_len_target, BATCH_SIZE, tgt_vocab_size)\n",
        "    if target_idx is None:\n",
        "      return ys, None\n",
        "    # Cross_entropy requires the \"Class\" dimension to be the 2nd dimension\n",
        "    T, B, C = y.shape\n",
        "    y = y.view(B*T, C)\n",
        "    target_idx = target_idx.view(B*T)\n",
        "    # Calculate loss\n",
        "    loss = F.cross_entropy(y, target_idx, ignore_index=toks_encode([PAD_TOKEN], \"target\")[0])\n",
        "    return ys, loss\n",
        "\n",
        "  def encode(self, input_batch: torch.Tensor):\n",
        "    # Input batch is of shape (B, T) (i.e. (batch size, block_size))\n",
        "    token_emb = self.encoder_token_emb(input_batch) # (B, T, E) where E=emb_size\n",
        "    # the position_embedding_table takes input the position of each token in the sequence (i.e. the T dimension)\n",
        "    position_emb = self.positional_embedding # (T, E)\n",
        "    x = token_emb + position_emb # (B, T, E)\n",
        "    assert x.shape == (BATCH_SIZE, BLOCK_SIZE, self.emb_size)\n",
        "    # Feed this x through layers of Transformer Self-Attention blocks\n",
        "    x = self.encoder_layers(x)\n",
        "    return x\n",
        "\n",
        "  def decode(self, context_emb: torch.Tensor, input_tokenIds: torch.Tensor, prev_hidden_state: torch.Tensor):\n",
        "    \"\"\"Decode the logis from the encoder to produce a target token\"\"\"\n",
        "    # Right now let's use an RNN-like decoder\n",
        "    assert context_emb.shape == (BATCH_SIZE, self.emb_size)\n",
        "    assert input_tokenIds.shape == (BATCH_SIZE,)\n",
        "    assert prev_hidden_state.shape == (BATCH_SIZE, self.decoder_hidden_state_size), f\"Got {prev_hidden_state.shape}\"\n",
        "    # For the decoder, we try to replicate the RNN model to process sequences\n",
        "    # decoder_hidden_state = tanh(W1 * context_matrix + W2 * prev_hidden_state + W3 * decoder_input_tok_emb + bias)\n",
        "    temp1 = self.decoder_context_linear(context_emb) # (B, dec_hidden_size)\n",
        "    temp2 = self.decoder_hiddenstate_linear(prev_hidden_state) # (B, dec_hiden_size)\n",
        "    tok_emb = self.decoder_token_emb(input_tokenIds) # (B, E)\n",
        "    temp3 = self.decoder_token_emb_linear(tok_emb) # (B, dec_hidden_size)\n",
        "    z = temp1 + temp2 + temp3 # (B, dec_hidden_size)\n",
        "    hidden_state = torch.tanh(z) # (B, dec_hidden_size)\n",
        "    assert hidden_state.shape == (BATCH_SIZE, self.decoder_hidden_state_size)\n",
        "\n",
        "    tgt_distribution: torch.Tensor = self.tgt_lm_head(hidden_state) # (B, tgt_vocab_size)\n",
        "    # Do NOT run softmax here as the Pytorch Cross Entropy Loss function expects unnormalized numbers\n",
        "    # tgt_probs = F.softmax(tgt_distribution, dim=-1) # (B, tgt_vocab_size)\n",
        "    return hidden_state, tgt_distribution\n",
        "\n",
        "  def generate(self, input_idx: torch.Tensor, max_generated_tokens: int = 20):\n",
        "    with torch.no_grad():\n",
        "      encoder_last_hidden_state = self.encode(input_idx) # (B, T, E)\n",
        "      # Average all input tokens embs across the encoder last hidden state as the context\n",
        "      context_emb = torch.mean(encoder_last_hidden_state, dim=1) # (B, E)\n",
        "      chosen_tokens = torch.tensor(toks_encode([\"<START>\" for _ in range(BATCH_SIZE)], \"target\"), device=DEVICE) # (B)\n",
        "      first_batch_predicted_tokens: list[int] = []\n",
        "      # Initialize the first hidden state as 0s\n",
        "      hidden_state = torch.zeros((BATCH_SIZE, self.decoder_hidden_state_size), device=DEVICE) # (B, H) where H = dec_hidden_state\n",
        "      for _ in range(max_generated_tokens):\n",
        "        # hidden_state: dimension (B, H)\n",
        "        # tgt_probs: dimension (B, tgt_vocab_size)\n",
        "        hidden_state, tgt_probs = self.decode(context_emb, chosen_tokens, prev_hidden_state=hidden_state)\n",
        "        # Greedily select the token with highest prob from the distribution\n",
        "        chosen_tokens = torch.argmax(tgt_probs, dim=1) # (B)\n",
        "        # print(chosen_tokens)\n",
        "        chosen_token = chosen_tokens[0].item() # View result of only the first batch\n",
        "        first_batch_predicted_tokens.append(int(chosen_token))\n",
        "    return first_batch_predicted_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "ePsttWMwVZi-",
        "outputId": "6482c9ea-9b66-403f-e64c-cfde5cb03dc6"
      },
      "outputs": [],
      "source": [
        "m1 = Transformer1()\n",
        "m1 = m1.to(DEVICE)\n",
        "\n",
        "# m1.eval()\n",
        "# x_batch = next(iter(train_dataloader))\n",
        "\n",
        "# input = torch.cat((x_batch[0], x_batch[1], x_batch[2]), dim=1) # (B, block_size)\n",
        "# print(\"START GENERATING\")\n",
        "# first_batch_predicted_tokens = m1.generate(input)\n",
        "# toks_decode(first_batch_predicted_tokens, \"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-3_lbn8TImMl"
      },
      "outputs": [],
      "source": [
        "# %%script echo skipping\n",
        "# Train the network\n",
        "# Create an optimizer\n",
        "m1.train()\n",
        "optimizer = torch.optim.AdamW(m1.parameters(), lr=5e-5)\n",
        "train_dataloader_iter = iter(train_dataloader)\n",
        "\n",
        "for epoch in range(30):\n",
        "  try:\n",
        "    batch = next(train_dataloader_iter)\n",
        "  except StopIteration:\n",
        "    # Reset the dataloader\n",
        "    train_dataloader_iter = iter(train_dataloader)\n",
        "    batch = next(train_dataloader_iter)\n",
        "  input = torch.cat((batch[0], batch[1], batch[2]), dim=1) # (B, block_size)\n",
        "  target = batch[3] # (B, max_len_target)\n",
        "  ys, loss = m1(input, target)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(epoch, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JrjMkTStimxt"
      },
      "outputs": [],
      "source": [
        "# %%script echo skipping\n",
        "# After training\n",
        "# Save the model\n",
        "torch.save(m1.state_dict(), \"transform1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1251, 1286, 636, 1265, 5098, 4637, 5433, 2749, 2856, 4683, 1251, 866, 4683, 2749, 811, 5098, 4637, 811, 4989, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 5153, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952]\n",
            "['SELECT', 'dept_name', ',', 'building', 'FROM', 'department', 'WHERE', 'budget', '>', '(', 'SELECT', 'avg', '(', 'budget', ')', 'FROM', 'department', ')', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma']\n",
            "['CLS', 'text', 'build', 'CLS', 'text', 'room', 'number', 'CLS', 'number', 'capacity', 'CLS', 'text', 'department', 'name', 'CLS', 'text', 'build', 'CLS', 'number', 'budget', 'CLS', 'text', 'course', 'id', 'CLS', 'text', 'title', 'CLS', 'text', 'department', 'name', 'CLS', 'number', 'credit', 'CLS', 'text', 'id', 'CLS', 'text', 'name', 'CLS', 'text', 'department', 'name', 'CLS', 'number', 'salary', 'CLS', 'text', 'course', 'id', 'CLS', 'text', 'section', 'id', 'CLS', 'text', 'semester', 'CLS', 'number', 'year', 'CLS', 'text', 'build', 'CLS', 'text', 'room', 'number', 'CLS', 'text', 'time', 'slot', 'id', 'CLS', 'text', 'id', 'CLS', 'text', 'course', 'id', 'CLS', 'text', 'section', 'id', 'CLS', 'text', 'semester', 'CLS', 'number', 'year', 'CLS', 'text', 'id', 'CLS', 'text', 'name', 'CLS', 'text', 'department', 'name', 'CLS', 'number', 'total', 'credit', 'CLS', 'text', 'id', 'CLS', 'text', 'course', 'id', 'CLS', 'text', 'section', 'id', 'CLS', 'text', 'semester', 'CLS', 'number', 'year', 'CLS', 'text', 'grade', 'CLS', 'text', 'student', 'id', 'CLS', 'text', 'instructor', 'id', 'CLS', 'text', 'time', 'slot', 'id', 'CLS', 'text', 'day', 'CLS', 'number', 'start', 'hour', 'CLS', 'number', 'start', 'minute', 'CLS', 'number', 'end', 'hour', 'CLS', 'number', 'end', 'minute', 'CLS', 'text', 'course', 'id', 'CLS', 'text', 'prerequisite', 'id', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'CLS', 'classroom', 'CLS', 'department', 'CLS', 'course', 'CLS', 'instructor', 'CLS', 'section', 'CLS', 'teach', 'CLS', 'student', 'CLS', 'take', 'class', 'CLS', 'advisor', 'CLS', 'time', 'slot', 'CLS', 'prerequisite', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'Give', 'the', 'name', 'and', 'building', 'of', 'the', 'departments', 'with', 'greater', 'than', 'average', 'budget', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "[3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952, 3952]\n",
            "91\n",
            "['Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma', 'Sonoma']\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "m1_trained = Transformer1()\n",
        "m1_trained.load_state_dict(torch.load(\"transform1.pt\"))\n",
        "m1_trained = m1_trained.to(DEVICE)\n",
        "m1_trained.eval()\n",
        "\n",
        "x_batch = next(iter(train_dataloader))\n",
        "\n",
        "input = torch.cat((x_batch[0], x_batch[1], x_batch[2]), dim=1) # (B, block_size)\n",
        "target = x_batch[3].tolist()[0]\n",
        "print(target)\n",
        "print(toks_decode(target, \"target\"))\n",
        "\n",
        "y_batch = m1_trained.generate(input, max_generated_tokens=max_len_target)\n",
        "print(toks_decode(input.tolist()[0], \"source\"))\n",
        "print(y_batch)\n",
        "print(len(y_batch))\n",
        "print(toks_decode(y_batch, \"target\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
