{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGMyKMCDzRfe"
      },
      "outputs": [],
      "source": [
        "!pip install gdown torch torchtext torchdata stanza spacy --quiet\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kXJ5VJX25HYq"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "import spacy\n",
        "# Prepare Stanford's Stanza to tokenize the columns and tables names\n",
        "# stanza.download('en')\n",
        "# stanza_nlp = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='en')\n",
        "\n",
        "# Prepare spacy to tokenize and lemmatize the table names and columns names\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\", \"parser\"])\n",
        "# Fix the \"Spacy breaks up 'id' into 'i' and 'd' issue\" https://github.com/explosion/spaCy/discussions/10570\n",
        "spacy_nlp.tokenizer.rules = { key: value for key, value in spacy_nlp.tokenizer.rules.items() if key != \"id\" }\n",
        "\n",
        "DEVICE = \"mps\" # Apple Metal Shader (M1 chip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VKMceBe22STs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "spider_url = 'https://drive.google.com/u/0/uc?id=1iRDVHLr4mX2wQKSgA9J8Pire73Jahh0m&export=download' # https://yale-lily.github.io/spider\n",
        "output = 'spider.zip'\n",
        "if not os.path.exists(output):\n",
        "  gdown.download(spider_url, output, quiet=False)\n",
        "\n",
        "!if [ ! -d \"spider\" ]; then unzip spider.zip -d .; else echo \"The 'spider' folder already exists.\"; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz75rWXgzBRd",
        "outputId": "280a2d12-e04a-4388-906a-aa9655690682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column(db_id='perpetrator', table_index=0, col_index=1, name='perpetrator id', name_toks=['perpetrator', 'id'], orig_name='Perpetrator_ID', col_type='number')\n",
            "Database(db_id='entrepreneur', raw_schema={'column_names': [[-1, '*'], [0, 'entrepreneur id'], [0, 'people id'], [0, 'company'], [0, 'money requested'], [0, 'investor'], [1, 'people id'], [1, 'name'], [1, 'height'], [1, 'weight'], [1, 'date of birth']], 'column_names_original': [[-1, '*'], [0, 'Entrepreneur_ID'], [0, 'People_ID'], [0, 'Company'], [0, 'Money_Requested'], [0, 'Investor'], [1, 'People_ID'], [1, 'Name'], [1, 'Height'], [1, 'Weight'], [1, 'Date_of_Birth']], 'column_types': ['text', 'number', 'number', 'text', 'number', 'text', 'number', 'text', 'number', 'number', 'text'], 'db_id': 'entrepreneur', 'foreign_keys': [[2, 6]], 'primary_keys': [1, 6], 'table_names': ['entrepreneur', 'people'], 'table_names_original': ['entrepreneur', 'people']}, tables=[Table(db_id='entrepreneur', table_index=0, name='entrepreneur', name_toks=['entrepreneur'], orig_name='entrepreneur', columns=[Column(db_id='entrepreneur', table_index=0, col_index=1, name='entrepreneur id', name_toks=['entrepreneur', 'id'], orig_name='Entrepreneur_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=2, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=3, name='company', name_toks=['company'], orig_name='Company', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=4, name='money requested', name_toks=['money', 'request'], orig_name='Money_Requested', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=5, name='investor', name_toks=['investor'], orig_name='Investor', col_type='text')]), Table(db_id='entrepreneur', table_index=1, name='people', name_toks=['people'], orig_name='people', columns=[Column(db_id='entrepreneur', table_index=1, col_index=6, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=7, name='name', name_toks=['name'], orig_name='Name', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=8, name='height', name_toks=['height'], orig_name='Height', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=9, name='weight', name_toks=['weight'], orig_name='Weight', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=10, name='date of birth', name_toks=['date', 'of', 'birth'], orig_name='Date_of_Birth', col_type='text')])], columns=[Column(db_id='entrepreneur', table_index=-1, col_index=0, name='*', name_toks=['*'], orig_name='*', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=1, name='entrepreneur id', name_toks=['entrepreneur', 'id'], orig_name='Entrepreneur_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=2, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=3, name='company', name_toks=['company'], orig_name='Company', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=4, name='money requested', name_toks=['money', 'request'], orig_name='Money_Requested', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=5, name='investor', name_toks=['investor'], orig_name='Investor', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=6, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=7, name='name', name_toks=['name'], orig_name='Name', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=8, name='height', name_toks=['height'], orig_name='Height', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=9, name='weight', name_toks=['weight'], orig_name='Weight', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=10, name='date of birth', name_toks=['date', 'of', 'birth'], orig_name='Date_of_Birth', col_type='text')])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, Any, Union, List\n",
        "\n",
        "# Start by loading in the databases, tables, and columns schemas\n",
        "\n",
        "with open(\"spider/tables.json\") as f:\n",
        "  raw_schema_data = json.load(f)\n",
        "\n",
        "ColumnType = Literal[\"time\", \"text\", \"number\", \"boolean\", \"others\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Column:\n",
        "  db_id: str  # Which database this col belongs to\n",
        "  table_index: int  # Which specific table in the db this col belongs to\n",
        "  col_index: int  # Which specific column in this table\n",
        "  name: str\n",
        "  name_toks: list[str]  # Tokenized name\n",
        "  orig_name: str\n",
        "  col_type: ColumnType\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Table:\n",
        "  db_id: str  # Which database this table belongs to\n",
        "  table_index: int  # Which specific table in this database\n",
        "  name: str\n",
        "  name_toks: list[str]  # Tokenized name\n",
        "  orig_name: str\n",
        "  columns: list[Column] = field(default_factory=list)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Database:\n",
        "  db_id: str\n",
        "  raw_schema: Any  # The original raw schema from json file\n",
        "  tables: list[Table] = field(default_factory=list)\n",
        "  columns: list[Column] = field(default_factory=list)\n",
        "\n",
        "\n",
        "schema_lookup: dict[str, Database] = {}  # Map the db_id to that database's schema\n",
        "\n",
        "raw_schema_data[0].keys()\n",
        "# ['column_names', 'column_names_original', 'column_types', 'db_id',\n",
        "# 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original']\n",
        "\n",
        "\n",
        "def lemmatized_tokenizer(s: str):\n",
        "  doc = spacy_nlp(s)\n",
        "  lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
        "  # doc = stanza_nlp(s)\n",
        "  # lemmatized_tokens = [word.lemma.lower() for sent in doc.sentences  for word in sent.words]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "\n",
        "for i, db_info in enumerate(raw_schema_data):\n",
        "  db_id = db_info[\"db_id\"]\n",
        "  db = Database(db_id, raw_schema=db_info)\n",
        "\n",
        "  # Iterate the tables of this database\n",
        "  for table_index, (tbl_name, tbl_orig_name) in enumerate(\n",
        "    zip(db_info[\"table_names\"], db_info[\"table_names_original\"])\n",
        "  ):\n",
        "    new_table = Table(\n",
        "      db_id,\n",
        "      table_index=table_index,\n",
        "      name=tbl_name,\n",
        "      name_toks=lemmatized_tokenizer(tbl_name),\n",
        "      orig_name=tbl_orig_name,\n",
        "    )\n",
        "    db.tables.append(new_table)\n",
        "\n",
        "  # Iterate the columns of this database\n",
        "  for col_index, ((table_idx, col_name), (_, col_orig_name), col_type) in enumerate(\n",
        "    zip(\n",
        "        db_info[\"column_names\"],\n",
        "        db_info[\"column_names_original\"],\n",
        "        db_info[\"column_types\"],\n",
        "    )\n",
        "  ):\n",
        "    new_col = Column(\n",
        "        db_id=db_id,\n",
        "        col_index=col_index,\n",
        "        col_type=col_type,\n",
        "        table_index=table_idx,\n",
        "        name=col_name,\n",
        "        name_toks=lemmatized_tokenizer(col_name),\n",
        "        orig_name=col_orig_name,\n",
        "    )\n",
        "    db.columns.append(new_col)\n",
        "    table_idx: int = table_idx\n",
        "    if table_idx >= 0:  # Skip the -1 index table\n",
        "        db.tables[table_idx].columns.append(new_col)\n",
        "\n",
        "  schema_lookup[db_id] = db\n",
        "\n",
        "all_columns = [col for s in schema_lookup.values() for col in s.columns]\n",
        "all_tables = [tbl for s in schema_lookup.values() for tbl in s.tables]\n",
        "\n",
        "print(all_columns[1])\n",
        "print(schema_lookup[\"entrepreneur\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "collapsed": true,
        "id": "wv5MghOvMVqh"
      },
      "outputs": [],
      "source": [
        "# Load the questions\n",
        "with open(\"spider/train_spider.json\") as f:\n",
        "  raw_train_data = json.load(f)\n",
        "\n",
        "with open(\"spider/dev.json\") as f:\n",
        "  raw_val_data = json.load(f)\n",
        "\n",
        "raw_train_data[0].keys()\n",
        "# ['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql']\n",
        "\n",
        "@dataclass\n",
        "class QAPair: # Class to encapsulate each NL query with corresponding SQL answer\n",
        "  db_id: str\n",
        "  question_toks: list[str]\n",
        "  question: str\n",
        "  query_toks: list[str]\n",
        "  query: str\n",
        "  sql_tree: Any # Raw sql tree as a nested dict from the raw JSON\n",
        "\n",
        "@dataclass\n",
        "class SpiderItem:\n",
        "  \"\"\"An item to feed into the neural network. Basically encapsulation of a training example\"\"\"\n",
        "  db_id: str\n",
        "  qa_pair: QAPair\n",
        "  db: Database # Information about the database used in the qa\n",
        "\n",
        "\n",
        "train_qas: list[QAPair] = []\n",
        "val_qas: list[QAPair] = []\n",
        "train_items: list[SpiderItem] = []\n",
        "val_items: list[SpiderItem] = []\n",
        "\n",
        "for i, qa_info in enumerate(raw_train_data):\n",
        "  db_id = qa_info[\"db_id\"]\n",
        "  new_qa = QAPair(\n",
        "    db_id=db_id,\n",
        "    question_toks=qa_info[\"question_toks\"],\n",
        "    question=qa_info[\"question\"],\n",
        "    query_toks=qa_info[\"query_toks\"],\n",
        "    query=qa_info[\"query\"],\n",
        "    sql_tree=qa_info[\"sql\"]\n",
        "  )\n",
        "  new_item = SpiderItem(\n",
        "    db_id=db_id,\n",
        "    qa_pair=new_qa,\n",
        "    db=schema_lookup[db_id]\n",
        "  )\n",
        "  train_qas.append(new_qa)\n",
        "  train_items.append(new_item)\n",
        "\n",
        "for i, qa_info in enumerate(raw_val_data):\n",
        "  db_id = qa_info[\"db_id\"]\n",
        "  new_qa = QAPair(\n",
        "    db_id=db_id,\n",
        "    question_toks=qa_info[\"question_toks\"],\n",
        "    question=qa_info[\"question\"],\n",
        "    query_toks=qa_info[\"query_toks\"],\n",
        "    query=qa_info[\"query\"],\n",
        "    sql_tree=qa_info[\"sql\"]\n",
        "  )\n",
        "  new_item = SpiderItem(\n",
        "    db_id=db_id,\n",
        "    qa_pair=new_qa,\n",
        "    db=schema_lookup[db_id]\n",
        "  )\n",
        "  val_qas.append(new_qa)\n",
        "  val_items.append(new_item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK7Sg2WoIT2C",
        "outputId": "556adaf2-cd93-43ac-9057-d154b9da4943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'boolean', 'time', 'text', 'number', 'others'}\n",
            "Vocab size col names 1176\n",
            "Vocab size tbl names 506\n",
            "['regular', 'performance', 'dimension', 'uid', 'year', '600', 'uk', 'mark', 'student', 'key']\n",
            "['regular', 'performance', 'key', 'student', 'chip', 'breed', 'author', 'show', 'channel', 'culture']\n"
          ]
        }
      ],
      "source": [
        "col_name_vocab: set[str] = set()\n",
        "tbl_name_vocab: set[str] = set()\n",
        "col_types: set[str] = set()\n",
        "\n",
        "# Build col names and tbl names vocab\n",
        "for col in all_columns:\n",
        "  col_name_vocab.update(col.name_toks)\n",
        "  col_types.add(col.col_type)\n",
        "\n",
        "for tbl in all_tables:\n",
        "  tbl_name_vocab.update(tbl.name_toks)\n",
        "\n",
        "print(col_types) # {'text', 'others', 'number', 'time', 'boolean'}\n",
        "print(\"Vocab size col names\", len(col_name_vocab))\n",
        "print(\"Vocab size tbl names\", len(tbl_name_vocab))\n",
        "print(list(col_name_vocab)[0:10])\n",
        "print(list(tbl_name_vocab)[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFKZz5rdySzh",
        "outputId": "0c14015e-1c57-4854-f94c-5af7431a1673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size questions 3750\n",
            "Vocab size SQL 5468\n"
          ]
        }
      ],
      "source": [
        "target_vocab: set[str] = set() # Vocab of the SQL target\n",
        "question_vocab: set[str] = set() # Vocab of the Natural Language questions\n",
        "\n",
        "for qa_pair in train_qas:\n",
        "  question_vocab.update(qa_pair.question_toks)\n",
        "  target_vocab.update(qa_pair.query_toks)\n",
        "\n",
        "for qa_pair in val_qas:\n",
        "  question_vocab.update(qa_pair.question_toks)\n",
        "  target_vocab.update(qa_pair.query_toks)\n",
        "\n",
        "# Padding token used to pad shorter inputs so every sequence of inputs to the encoder\n",
        "# is of same length (which would be the max length)\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "src_vocab: set[str] = set() # Source vocab used for inputs to the encoder, which includes questions & columns & tables names\n",
        "src_vocab.update(col_name_vocab)\n",
        "src_vocab.update(col_types)\n",
        "src_vocab.update(tbl_name_vocab)\n",
        "src_vocab.update(question_vocab)\n",
        "# Add the \"CLS\" token as a separator between each part in the input block (i.e. separator between each column name, table name, etc.)\n",
        "src_vocab.add(\"CLS\")\n",
        "src_vocab.add(PAD_TOKEN)\n",
        "\n",
        "tgt_vocab: set[str] = set() # Target vocab used for decoder output; which is the set of SQL tokens plus the Start and End tokens\n",
        "tgt_vocab.update(target_vocab)\n",
        "tgt_vocab.add(\"<START>\")\n",
        "tgt_vocab.add(\"<END>\")\n",
        "tgt_vocab.add(PAD_TOKEN)\n",
        "\n",
        "print(\"Vocab size questions\", len(question_vocab))\n",
        "print(\"Vocab size SQL\", len(target_vocab))\n",
        "\n",
        "# Token to id: Assign a number to each token in the vocab\n",
        "# Id to token: Retrieve the token give the index\n",
        "src_tok_to_idx = { tok: i for i, tok in enumerate(src_vocab) }\n",
        "src_idx_to_tok = { i: tok for i, tok in enumerate(src_vocab) }\n",
        "tgt_tok_to_idx = { tok: i for i, tok in enumerate(tgt_vocab) }\n",
        "tgt_idx_to_tok = { i: tok for i, tok in enumerate(tgt_vocab) }\n",
        "\n",
        "def toks_encode(tokens_list: list[str], mode: Literal[\"source\", \"target\"]):\n",
        "  \"\"\"Given a list of tokens, 'encode' them to return a list of token_ids\n",
        "  Parameters:\n",
        "    mode: Whether to use the source's or target's tok_to_idx lookup table\n",
        "  \"\"\"\n",
        "  if mode == \"source\":\n",
        "    return [src_tok_to_idx[token] for token in tokens_list]\n",
        "  return [tgt_tok_to_idx[token] for token in tokens_list]\n",
        "\n",
        "def toks_decode(ids_list: list[int], mode: Literal[\"source\", \"target\"]):\n",
        "  \"\"\"Given a list of token_ids, 'decode' them to return the list of corresponding token\n",
        "  Parameters:\n",
        "    mode: Whether to use the source's or target's idx_to_tok lookup table\n",
        "  \"\"\"\n",
        "  if mode == \"source\":\n",
        "    return [src_idx_to_tok[token_id] for token_id in ids_list]\n",
        "  return [tgt_idx_to_tok[token_id] for token_id in ids_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzijLcv7E5F8",
        "outputId": "7e915550-7778-497d-f91d-0b3ff2878aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7000\n",
            "1034\n",
            "[1196, 70, 44, 92]\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset and dataloader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def construct_input_block_toks(\n",
        "    col_toks: list[list[str]],\n",
        "    col_types: list[ColumnType],\n",
        "    table_toks: list[list[str]],\n",
        "    ques_toks: list[str],\n",
        "):\n",
        "    \"\"\"Construct and return the 3 components of an input block as a tuple of the list of tokens\n",
        "    An input block has 3 parts:\n",
        "      The col name part: `CLS col_type ...col_name CLS col_type2 ...Col_name2`\n",
        "      The table name part: `CLS ...tbl_name CLS ...tbl_name2`\n",
        "      The question part: ` CLS ...question`\n",
        "\n",
        "    Parameters:\n",
        "      col_toks: nested list of all tokens across all column names, each element is a list of tokens that belongs in one column name\n",
        "      tbl_toks: nested list of all tokens across all table names, each element is a list of tokens that belongs in one table name\n",
        "\n",
        "    Returns:\n",
        "      a 3-elemment tuple containing (col_name_part, tbl_name_part, ques_part) w/ each part being the list of tokens including the 'CLS' token\n",
        "    \"\"\"\n",
        "    # List of column types should match length with list of columns\n",
        "    assert len(col_toks) == len(col_types)\n",
        "    # Construct the col name part, thus producing something like:\n",
        "    # \"CLS number builing size CLS text building name...\"\n",
        "    col_name_part: list[str] = []\n",
        "    for col_name_toks, col_type in zip(col_toks, col_types):\n",
        "        col_name_part.extend([\"CLS\", col_type] + col_name_toks)\n",
        "    # Construct the col name part, thus producing something like:\n",
        "    # \"CLS building information CLS city information ...etc\"\n",
        "    tbl_name_part: list[str] = []\n",
        "    for tbl_name_toks in table_toks:\n",
        "        tbl_name_part.extend([\"CLS\"] + tbl_name_toks)\n",
        "    # Construct the question part, thus producing something like:\n",
        "    # \"CLS what is the highest building in Chicago\"\n",
        "    question_part: list[str] = ques_toks\n",
        "    return (col_name_part, tbl_name_part, question_part)\n",
        "\n",
        "DatasetItem = tuple[list[str], list[str], list[str], list[str]]\n",
        "class MyDataset(Dataset[DatasetItem]):\n",
        "    \"\"\"Each element in this dataset is a 4-element tuple\n",
        "    (col_name_toks, table_name_toks, question_toks, target_toks)\n",
        "\n",
        "    where `col_name_toks`, `table_name_toks`, `question_toks` has been preprocessed to add \"CLS\" separator token\n",
        "    and `target_toks` is also processed to include <Start> and <End> tokens\n",
        "    \"\"\"\n",
        "    def __init__(self, all_items: list[SpiderItem]):\n",
        "        self.items = all_items\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> DatasetItem:\n",
        "        item = self.items[idx]\n",
        "        columns_tokens = [c.name_toks for c in item.db.columns[1:]] # Skip the 0th-index \"*\" column\n",
        "        columns_types: list[ColumnType] = [c.col_type for c in item.db.columns[1:]]\n",
        "        tables_tokens = [t.name_toks for t in item.db.tables]\n",
        "        question_tokens = item.qa_pair.question_toks\n",
        "        target_tokens = [\"<START>\"] + item.qa_pair.query_toks + [\"<END>\"]\n",
        "\n",
        "        return construct_input_block_toks(columns_tokens, columns_types, tables_tokens, question_tokens) + (target_tokens,)\n",
        "\n",
        "\n",
        "train_dataset = MyDataset(train_items)\n",
        "val_dataset = MyDataset(val_items)\n",
        "print(len(train_dataset))  # Should be 7000\n",
        "print(len(val_dataset)) # Should be 1034\n",
        "\n",
        "PAD_TOKEN_ID = src_tok_to_idx[PAD_TOKEN]\n",
        "\n",
        "# Find the max_length of each \"category\".\n",
        "# Category here correspondings to the 4 categories (col_name_toks, table_name_toks, question_toks, target_toks)\n",
        "max_lengths = [max(len(category) for category in item) for item in zip(*train_dataset)]\n",
        "print(max_lengths) # [1196, 70, 44, 92]\n",
        "max_len_col_name_part = max_lengths[0]\n",
        "max_len_tbl_name_part = max_lengths[1]\n",
        "max_len_ques_part = max_lengths[2]\n",
        "max_len_target = max_lengths[3]\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def pad_collate(batch: list[tuple[list[str], list[str], list[str], list[str]]]):\n",
        "    \"\"\"The collate_fn to pass to Pytorch Dataloader.\n",
        "    Pad each element in the dataset so they all have the same size\"\"\"\n",
        "    batched_col_name_toks, batched_tbl_name_toks, batched_ques_toks, batched_target_toks = zip(*batch)\n",
        "\n",
        "    batched_colname_tokIds = [torch.tensor(toks_encode(col_name_toks, mode=\"source\"), dtype=torch.long) for col_name_toks in batched_col_name_toks]\n",
        "    # Now pad this batch so that every item of the batch is same size as a local longest item\n",
        "    # From the doc: `pad_sequence` stacks a list of Tensors along a new dimension, and pads them to equal length.\n",
        "    batched_colname_tokIds = pad_sequence(batched_colname_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    # Now pad this batch again so that every item of the batch is same size of the global longest item (max_len_col_name_part)\n",
        "    batched_colname_tokIds= torch.nn.functional.pad(\n",
        "        batched_colname_tokIds, (0, max_len_col_name_part - batched_colname_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_colname_tokIds = batched_colname_tokIds.to(DEVICE) # (B, T)\n",
        "\n",
        "    # After padding, each tensor should be dimension (B, T)\n",
        "    # With B being the batch dimension (since we use batch_first=True) and T is max_len_col_name_part\n",
        "    assert batched_colname_tokIds.shape == (BATCH_SIZE, max_len_col_name_part), f\"Expected {BATCH_SIZE, max_len_col_name_part}, got {batched_colname_tokIds.shape}\"\n",
        "\n",
        "    # Do the same process for table name toks, question toks, and target toks\n",
        "    batched_tblname_tokIds = [torch.tensor(toks_encode(tbl_name_toks, mode=\"source\")) for tbl_name_toks in batched_tbl_name_toks]\n",
        "    batched_tblname_tokIds = pad_sequence(batched_tblname_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    batched_tblname_tokIds= torch.nn.functional.pad(\n",
        "        batched_tblname_tokIds, (0, max_len_tbl_name_part - batched_tblname_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_tblname_tokIds = batched_tblname_tokIds.to(DEVICE)\n",
        "    assert batched_tblname_tokIds.shape == (BATCH_SIZE, max_len_tbl_name_part)\n",
        "\n",
        "    batched_ques_tokIds = [torch.tensor(toks_encode(ques_toks, mode=\"source\")) for ques_toks in batched_ques_toks]\n",
        "    batched_ques_tokIds = pad_sequence(batched_ques_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    batched_ques_tokIds = torch.nn.functional.pad(\n",
        "        batched_ques_tokIds, (0, max_len_ques_part - batched_ques_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_ques_tokIds = batched_ques_tokIds.to(DEVICE)\n",
        "    assert batched_ques_tokIds.shape == (BATCH_SIZE, max_len_ques_part)\n",
        "\n",
        "    batched_tgt_tokIds = [torch.tensor(toks_encode(tgt_toks, mode=\"target\")) for tgt_toks in batched_target_toks]\n",
        "    # Note that for the target, we need use `tgt_tok_to_idx`\n",
        "    batched_tgt_tokIds = pad_sequence(batched_tgt_tokIds, batch_first=True, padding_value=tgt_tok_to_idx[PAD_TOKEN])\n",
        "    batched_tgt_tokIds = torch.nn.functional.pad(\n",
        "        batched_tgt_tokIds, (0, max_len_target - batched_tgt_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_tgt_tokIds = batched_tgt_tokIds.to(DEVICE)\n",
        "    assert batched_tgt_tokIds.shape == (BATCH_SIZE, max_len_target)\n",
        "\n",
        "    return (batched_colname_tokIds, batched_tblname_tokIds, batched_ques_tokIds, batched_tgt_tokIds)\n",
        "\n",
        "ModelInput = tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzuU6VkB3tuw"
      },
      "source": [
        "# Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C46jlVZY4H8A"
      },
      "source": [
        "## The Masked Attention Math trick\n",
        "\n",
        "The Triangular mask technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7PwwGDegHS2",
        "outputId": "9be7ecec-bcfc-42fa-ee7a-25933c2e0d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n"
          ]
        }
      ],
      "source": [
        "# The Attention Mask Math trick - The Triangular mask technique\n",
        "T = 300\n",
        "\n",
        "# A triangular mask is a tensor like this: dimension (block_size, block_size)\n",
        "# tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
        "#        [1., 1., 0.,  ..., 0., 0., 0.],\n",
        "#        [1., 1., 1.,  ..., 0., 0., 0.],\n",
        "#        ...,\n",
        "#        [1., 1., 1.,  ..., 1., 0., 0.],\n",
        "#        [1., 1., 1.,  ..., 1., 1., 0.],\n",
        "#        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
        "tril_mask = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# Initial weight represents the initial *affinity* between each pair of tokens in the sequence\n",
        "# (right now we initialize them all with 0s, but they can be diff numbers coming from somewhere like the encoder for example)\n",
        "initial_weight = torch.zeros((T, T))\n",
        "\n",
        "# For the purpose of decoder calculation, we want prediction of the next step to only depend on values of previous steps\n",
        "# Therefore, we'll apply a *triangular mask* to the weight vector\n",
        "# By the structure of the triangular mask, we can see that, after masking,\n",
        "# we essentially \"disable\"/\"mask away\" future tokens since they will be multiplying with 0\n",
        "# since only positions corresponding to 1s will be retained. Positions with 0s is replaced with -inf\n",
        "weight = initial_weight.masked_fill(tril_mask == 0, float(\"-inf\"))\n",
        "# A softmax layer convert these *affinity scores* to a weight number between 0 and 1\n",
        "weight = F.softmax(weight, dim=-1)\n",
        "print(weight)\n",
        "# tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        ...,\n",
        "#        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
        "#        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
        "#        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n",
        "# --> See how the weight vector showcase how to calculate a prediction based on the weighted sum of previous values only\n",
        "# For example:\n",
        "#   2nd value = 1.00 * 1st val\n",
        "#   3rd prediction = 0.5 * 1st val + 0.5 * 2nd val\n",
        "#   4th prediction = 0.33 * 1st value + 0.33 * 2nd val + 0.33 * 3rd val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNeqGOj4RSw"
      },
      "source": [
        "## Single head attention block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9xArcRAy4rkL"
      },
      "outputs": [],
      "source": [
        "# The dimension of the query, key, and value vectors/matrixes\n",
        "d_model = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Un6Q6p044kTN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1310\n"
          ]
        }
      ],
      "source": [
        "from math import sqrt\n",
        "\n",
        "BLOCK_SIZE = max_len_col_name_part + max_len_tbl_name_part + max_len_ques_part\n",
        "\n",
        "class SingleAttentionHead(nn.Module):\n",
        "  def __init__(self, emb_size: int, d_model: int):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.d_model = d_model\n",
        "    # Each head has its own W_Q, W_K, and W_V matrixes for transform the each tok emd to its corresponding q, k, v vectors\n",
        "    self.query_matrix = nn.Linear(emb_size, d_model, bias=False)\n",
        "    self.key_matrix = nn.Linear(emb_size, d_model, bias=False)\n",
        "    self.value_matrix = nn.Linear(emb_size, d_model, bias=False)\n",
        "    # tril_mask is a static non-learned parameter, so need to use `register_buffer`\n",
        "    self.register_buffer(\"tril_mask\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "  def forward(self, x: torch.Tensor, should_mask:bool=True):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      x: the input embedding (after summing token_emb and position_emb) --> dimension (B, T, E)\n",
        "      should_mask: should this Attention block use masked attention (decoder should use mask, encoder shouldn't)\n",
        "    \"\"\"\n",
        "    query = self.query_matrix(x) # (B, T, D) with D = d_model\n",
        "    key = self.key_matrix(x) # (B, T, D)\n",
        "    value = self.value_matrix(x) # (B, T, D)\n",
        "    # `q @ k.T` will produce the itcanitial affinity matrix, basically how strong each query relates to each key\n",
        "    # dimension: (B, T, D) @ (B, D, T) = (B, T, T)\n",
        "    # Note The original \"Attention is all you need paper\" also scales down the affinity scores by multiplying `sqrt(d_model)`\n",
        "    affinity = (query @ key.transpose(-2, -1)) * (sqrt(d_model))  # tranpose(-2, -1) avoid transposing the Batch dimension\n",
        "    if should_mask:\n",
        "      affinity = affinity.masked_fill(self.tril_mask == 0, float(\"-inf\"))\n",
        "    weight = F.softmax(affinity, dim=-1) # (B, T, T)\n",
        "    # The output is the inner embeddings (dimension d_model) where each token's embedding have been tweaked\n",
        "    # to also include information about other related tokens\n",
        "    out = weight @ value # (B, T, D)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JumzVuyT4X-f"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "quKt89aqF6zC"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "class Transformer1(nn.Module):\n",
        "  def __init__(self, emb_size: int = 300, d_model: int = 256):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      emb_size: the size of each word embeddings. For example: GloVe embeddings is 300, BERT is 768\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.d_model = d_model\n",
        "    self.self_attention_head = SingleAttentionHead(emb_size, d_model=d_model)\n",
        "    # ENCODER COMPONENTS\n",
        "    self.encoder_token_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "    # Position embedding table: Convert each token's position in its block to a position embedding\n",
        "    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, emb_size)\n",
        "\n",
        "    # DECODER COMPONENTS\n",
        "    self.decoder_hidden_state_size = 2 * d_model\n",
        "    # Embedding lookup table: Convert token_ids to that token's corresponding embeddings\n",
        "    self.decoder_token_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
        "    # Target language modeling head: Transform back from the embedding dimension to the tgt_vocab_size dimension\n",
        "    # So that we can get the distribution and know which target token to choose\n",
        "    self.tgt_lm_head = nn.Linear(self.decoder_hidden_state_size, tgt_vocab_size)\n",
        "    # For the decoder, we try to replicate the RNN model to process sequences\n",
        "    # decoder_hidden_state = sigmoid(W1 * context_matrix + W2 * prev_hidden_state + bias + W3 * decoder_input_tok_emb)\n",
        "    # Thus we need 3 weights matrix for use in the decoder, to produce a new decoder_hidden_state\n",
        "    self.decoder_context_linear = nn.Linear(d_model, self.decoder_hidden_state_size, bias=False)\n",
        "    self.decoder_hiddenstate_linear = nn.Linear(self.decoder_hidden_state_size, self.decoder_hidden_state_size)\n",
        "    self.decoder_token_emb_linear = nn.Linear(emb_size, self.decoder_hidden_state_size, bias=False)\n",
        "\n",
        "  def forward(self, input_idx: torch.Tensor, target_idx: Union[torch.Tensor, None] = None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      target_idx: the list of target tokens across the batches. Dimension (B, T)\n",
        "    \"\"\"\n",
        "    x = self.encode(input_idx) # (B, T, E)\n",
        "    # Average the last hidden state of the encoder as the context\n",
        "    context_emb = torch.mean(x, dim=1) # (B, E)\n",
        "    # Feed the <START> token as the chosen token to the entire batch\n",
        "    chosen_tokens = torch.tensor(toks_encode([\"<START>\"] * x.size(0), \"target\"), device=DEVICE) # (B)\n",
        "    ys: list[torch.Tensor] = []\n",
        "    # Initialize the first hidden state from the last hidden state from encoder\n",
        "    hidden_state = self.decoder_context_linear(context_emb) # (B, H) where H = dec_hidden_state\n",
        "    for _ in range(max_len_target):\n",
        "      # hidden_state: (B, H)\n",
        "      # tgt_probs: (B, tgt_vocab_size)\n",
        "      hidden_state, tgt_probs = self.decode(context_emb, chosen_tokens, hidden_state)\n",
        "      # Greedily select the token with highest prob from the distribution\n",
        "      chosen_tokens = torch.argmax(tgt_probs, dim=1) # (B)\n",
        "      ys.append(tgt_probs)\n",
        "\n",
        "    # Note that ys is collected by looping over max_len_target, so when stacked, the first dimension is max_len_target\n",
        "    y = torch.stack(ys) # (T, B, C) where C = tgt_vocab_size\n",
        "    assert y.shape == (max_len_target, BATCH_SIZE, tgt_vocab_size)\n",
        "    if target_idx is None:\n",
        "      return ys, None\n",
        "    # Cross_entropy requires the \"Class\" dimension to be the 2nd dimension\n",
        "    y = torch.permute(y, (1, 2, 0)) # --> (B, C, T)\n",
        "    B, C, T = y.shape\n",
        "    assert target_idx.size() == (B, T)\n",
        "    # Calculate loss\n",
        "    loss = F.cross_entropy(y, target_idx, ignore_index=toks_encode([PAD_TOKEN], \"target\")[0])\n",
        "    return ys, loss\n",
        "\n",
        "  def encode(self, input_batch: torch.Tensor):\n",
        "    # Input batch is of shape (B, T) (i.e. (batch size, block_size))\n",
        "    token_emb = self.encoder_token_emb(input_batch) # (B, T, E) where E=emb_size\n",
        "    B, T, E = token_emb.shape\n",
        "    # the position_embedding_table takes input the position of each token in the sequence (i.e. the T dimension)\n",
        "    position_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # (T, E)\n",
        "    # For the bigram model, let's not do anything with these embeddings, and just return them as logits\n",
        "    x = token_emb + position_emb # (B, T, E)\n",
        "    # Feed this x through layers of Transformer Self-Attention blocks\n",
        "    x = self.self_attention_head(x)\n",
        "    return x\n",
        "\n",
        "  def decode(self, context_emb: torch.Tensor, input_tokenIds: torch.Tensor, prev_hidden_state: torch.Tensor):\n",
        "    \"\"\"Decode the logis from the encoder to produce a target token\"\"\"\n",
        "    # Right now let's use an RNN-like decoder\n",
        "    assert context_emb.shape == (BATCH_SIZE, self.d_model)\n",
        "    assert input_tokenIds.shape == (BATCH_SIZE,)\n",
        "    assert prev_hidden_state.shape == (BATCH_SIZE, self.decoder_hidden_state_size), f\"Got {prev_hidden_state.shape}\"\n",
        "    tok_emb = self.decoder_token_emb(input_tokenIds) # (B, D) where D = d_model\n",
        "    assert tok_emb.shape == (BATCH_SIZE, self.emb_size)\n",
        "    # For the decoder, we try to replicate the RNN model to process sequences\n",
        "    # decoder_hidden_state = sigmoid(W1 * context_matrix + W2 * prev_hidden_state + W3 * decoder_input_tok_emb + bias)\n",
        "    temp1 = self.decoder_context_linear(context_emb) # (B, dec_hidden_size)\n",
        "    temp2 = self.decoder_hiddenstate_linear(prev_hidden_state) # (B, dec_hiden_size)\n",
        "    temp3 = self.decoder_token_emb_linear(tok_emb) # (B, dec_hidden_size)\n",
        "    z = temp1 + temp2 + temp3 # (B, dec_hidden_size)\n",
        "    hidden_state = F.sigmoid(z) # (B, dec_hidden_size)\n",
        "    assert hidden_state.shape == (BATCH_SIZE, self.decoder_hidden_state_size)\n",
        "\n",
        "    tgt_distribution: torch.Tensor = self.tgt_lm_head(hidden_state) # (B, tgt_vocab_size)\n",
        "    # Do NOT run softmax here as the Pytorch Cross Entropy Loss function expects unnormalized numbers\n",
        "    # tgt_probs = F.softmax(tgt_distribution, dim=-1) # (B, tgt_vocab_size)\n",
        "    return hidden_state, tgt_distribution\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, input_idx: torch.Tensor, max_generated_tokens: int = 20):\n",
        "    encoder_last_hidden_state = self.encode(input_idx) # (B, T, E)\n",
        "    # Average all input tokens embs across the encoder last hidden state as the context\n",
        "    context_emb = torch.mean(encoder_last_hidden_state, dim=1) # (B, E)\n",
        "    chosen_tokens = torch.tensor(toks_encode([\"<START>\"] * BATCH_SIZE, \"target\"), device=DEVICE) # (B)\n",
        "    first_batch_predicted_tokens: list[int] = []\n",
        "    # Initialize the first hidden state from the last hidden state from encoder\n",
        "    hidden_state = self.decoder_context_linear(context_emb) # (B, dec_hidden_state)\n",
        "    for _ in range(max_generated_tokens):\n",
        "      # hidden_state: dimension (B, H)\n",
        "      # tgt_probs: dimension (B, tgt_vocab_size)\n",
        "      hidden_state, tgt_probs = self.decode(context_emb, chosen_tokens, prev_hidden_state=hidden_state)\n",
        "      # Greedily select the token with highest prob from the distribution\n",
        "      chosen_tokens = torch.argmax(tgt_probs, dim=1) # (B)\n",
        "      # print(chosen_tokens)\n",
        "      chosen_token = chosen_tokens[0].item() # View result of only the first batch\n",
        "      first_batch_predicted_tokens.append(int(chosen_token))\n",
        "    return first_batch_predicted_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "ePsttWMwVZi-",
        "outputId": "6482c9ea-9b66-403f-e64c-cfde5cb03dc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['claims',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes',\n",
              " 'dateundergoes']"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m1 = Transformer1()\n",
        "m1 = m1.to(DEVICE)\n",
        "\n",
        "m1.eval()\n",
        "x_batch = next(iter(train_dataloader))\n",
        "\n",
        "input = torch.cat((x_batch[0], x_batch[1], x_batch[2]), dim=1) # (B, block_size)\n",
        "\n",
        "first_batch_predicted_tokens = m1.generate(input)\n",
        "toks_decode(first_batch_predicted_tokens, \"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "-3_lbn8TImMl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tensor(8.2893, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "1 tensor(8.3051, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "2 tensor(8.2695, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "3 tensor(8.3076, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "4 tensor(8.3521, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "5 tensor(8.2585, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "6 tensor(8.2283, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "7 tensor(8.2716, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "8 tensor(8.3237, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "9 tensor(8.2602, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "10 tensor(8.2499, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "11 tensor(8.2509, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "12 tensor(8.2351, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "13 tensor(8.2911, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "14 tensor(8.2242, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "15 tensor(8.1951, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "16 tensor(8.2210, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "17 tensor(8.1768, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "18 tensor(8.2244, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "19 tensor(8.1444, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "20 tensor(8.1524, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "21 tensor(8.1267, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "22 tensor(8.1221, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "23 tensor(8.1199, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "24 tensor(8.1131, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "25 tensor(8.0888, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "26 tensor(8.2199, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "27 tensor(8.0570, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "28 tensor(8.0863, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "29 tensor(8.5330, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "30 tensor(8.1462, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "31 tensor(8.0891, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "32 tensor(8.0882, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "33 tensor(8.0620, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "34 tensor(8.0147, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "35 tensor(8.0116, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "36 tensor(7.9785, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "37 tensor(8.0473, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "38 tensor(7.9919, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "39 tensor(8.0558, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "40 tensor(7.9494, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "41 tensor(7.9543, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "42 tensor(8.0289, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "43 tensor(8.0262, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "44 tensor(7.9730, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "45 tensor(7.9327, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "46 tensor(7.9566, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "47 tensor(7.9199, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "48 tensor(7.9558, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "49 tensor(7.9010, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "50 tensor(7.9631, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "51 tensor(7.9917, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "52 tensor(7.9005, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "53 tensor(8.0699, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "54 tensor(7.9112, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "55 tensor(7.9604, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "56 tensor(7.9740, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "57 tensor(7.8907, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "58 tensor(7.9075, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "59 tensor(7.8836, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "60 tensor(7.8212, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "61 tensor(7.8572, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "62 tensor(7.9162, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "63 tensor(7.8617, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "64 tensor(7.8496, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "65 tensor(7.8113, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "66 tensor(7.8272, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "67 tensor(7.9061, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "68 tensor(7.8272, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "69 tensor(7.7292, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "70 tensor(7.6981, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "71 tensor(7.7177, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "72 tensor(7.6815, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "73 tensor(7.8047, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "74 tensor(7.6740, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "75 tensor(7.6668, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "76 tensor(7.7605, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "77 tensor(7.7754, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "78 tensor(7.8177, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "79 tensor(7.6553, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "80 tensor(7.7762, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "81 tensor(7.7363, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "82 tensor(7.5971, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "83 tensor(7.5915, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "84 tensor(7.7247, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "85 tensor(7.6040, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "86 tensor(7.6906, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "87 tensor(7.8397, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "88 tensor(7.7728, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "89 tensor(7.5000, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "90 tensor(7.6228, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "91 tensor(7.6857, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "92 tensor(7.5481, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "93 tensor(7.5083, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "94 tensor(7.7652, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "95 tensor(8.4266, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "96 tensor(7.5698, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "97 tensor(7.5020, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "98 tensor(7.6318, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "99 tensor(7.5639, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "100 tensor(7.5615, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "101 tensor(7.5693, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "102 tensor(7.5050, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "103 tensor(7.5786, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "104 tensor(7.4317, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "105 tensor(7.4100, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "106 tensor(7.4720, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "107 tensor(7.4837, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "108 tensor(7.5281, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "109 tensor(7.4399, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "110 tensor(7.5905, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "111 tensor(7.4623, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "112 tensor(7.3397, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "113 tensor(7.4001, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "114 tensor(7.4202, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "115 tensor(7.4637, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "116 tensor(7.2630, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "117 tensor(7.3677, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "118 tensor(7.4053, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "119 tensor(7.3575, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "120 tensor(7.4219, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "121 tensor(7.4529, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "122 tensor(7.5383, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "123 tensor(7.2037, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "124 tensor(7.3797, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "125 tensor(7.3998, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "126 tensor(7.3114, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "127 tensor(7.2432, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "128 tensor(7.2846, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "129 tensor(7.1440, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "130 tensor(7.4439, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "131 tensor(7.0909, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "132 tensor(7.1543, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "133 tensor(7.1202, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "134 tensor(7.2762, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "135 tensor(7.1955, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "136 tensor(7.2241, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "137 tensor(7.1417, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "138 tensor(7.0075, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "139 tensor(7.0868, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "140 tensor(7.1639, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "141 tensor(7.0758, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "142 tensor(7.4852, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "143 tensor(7.1438, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "144 tensor(7.2689, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "145 tensor(7.1147, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "146 tensor(7.1564, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "147 tensor(7.3626, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "148 tensor(6.9881, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "149 tensor(7.0859, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "150 tensor(6.9985, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "151 tensor(7.1712, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "152 tensor(6.8837, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "153 tensor(6.9022, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "154 tensor(6.9000, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "155 tensor(7.0129, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "156 tensor(7.1488, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "157 tensor(6.9352, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "158 tensor(6.8997, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "159 tensor(7.3238, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "160 tensor(6.8227, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "161 tensor(6.8974, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "162 tensor(8.2680, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "163 tensor(6.7931, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "164 tensor(6.7747, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "165 tensor(7.1370, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "166 tensor(7.2238, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "167 tensor(6.9904, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "168 tensor(6.8860, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "169 tensor(6.9986, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "170 tensor(6.9755, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "171 tensor(7.0453, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "172 tensor(6.9041, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "173 tensor(6.9760, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "174 tensor(6.8115, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "175 tensor(6.8812, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "176 tensor(6.7676, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "177 tensor(6.8421, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "178 tensor(6.6725, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "179 tensor(6.6367, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "180 tensor(8.2312, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "181 tensor(6.5960, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "182 tensor(6.6655, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "183 tensor(6.6867, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "184 tensor(6.7853, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "185 tensor(6.5987, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "186 tensor(6.9020, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "187 tensor(7.0582, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "188 tensor(6.8117, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "189 tensor(6.5535, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "190 tensor(6.7442, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "191 tensor(6.8340, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "192 tensor(6.8196, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "193 tensor(6.5947, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "194 tensor(6.5354, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "195 tensor(6.7611, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "196 tensor(6.5310, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "197 tensor(6.6745, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "198 tensor(6.5297, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "199 tensor(6.4070, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "200 tensor(6.7751, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "201 tensor(6.5207, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "202 tensor(6.5031, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "203 tensor(6.6233, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "204 tensor(6.3584, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "205 tensor(6.6725, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "206 tensor(6.5984, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "207 tensor(6.6227, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "208 tensor(6.4600, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "209 tensor(6.6186, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "210 tensor(6.6873, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "211 tensor(6.3344, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "212 tensor(6.5301, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "213 tensor(6.5151, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "214 tensor(6.5637, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "215 tensor(6.3431, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "216 tensor(6.5377, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "217 tensor(6.2556, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "218 tensor(6.4959, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "219 tensor(6.5262, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "220 tensor(6.5334, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "221 tensor(6.6764, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "222 tensor(6.6056, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "223 tensor(6.1886, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "224 tensor(6.7151, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "225 tensor(6.5328, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "226 tensor(6.5071, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "227 tensor(6.4225, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "228 tensor(6.2403, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "229 tensor(6.7768, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "230 tensor(6.3358, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "231 tensor(6.3636, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "232 tensor(6.1356, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "233 tensor(6.4495, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "234 tensor(6.0549, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "235 tensor(6.4320, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "236 tensor(6.0587, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "237 tensor(6.4128, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "238 tensor(6.3753, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "239 tensor(6.3094, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "240 tensor(6.0510, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "241 tensor(6.1596, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "242 tensor(6.3597, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "243 tensor(6.1934, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "244 tensor(6.2998, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "245 tensor(6.4324, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "246 tensor(6.2505, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "247 tensor(6.7853, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "248 tensor(6.0733, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "249 tensor(6.1861, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "250 tensor(6.3190, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "251 tensor(5.9867, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "252 tensor(5.9665, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "253 tensor(6.1732, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "254 tensor(5.9889, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "255 tensor(6.0386, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "256 tensor(6.0450, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "257 tensor(6.3434, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "258 tensor(5.9927, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "259 tensor(6.1990, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "260 tensor(5.8835, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "261 tensor(5.8651, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "262 tensor(5.8751, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "263 tensor(5.9832, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "264 tensor(6.0342, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "265 tensor(5.9384, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "266 tensor(6.0956, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "267 tensor(5.8598, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "268 tensor(5.9611, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "269 tensor(6.1955, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "270 tensor(5.9181, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "271 tensor(6.1300, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "272 tensor(6.0518, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "273 tensor(6.2954, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "274 tensor(6.1800, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "275 tensor(5.7712, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "276 tensor(6.0974, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "277 tensor(6.0226, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "278 tensor(5.7687, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "279 tensor(6.1034, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "280 tensor(5.9831, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "281 tensor(5.7950, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "282 tensor(6.0367, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "283 tensor(6.0015, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "284 tensor(5.7442, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "285 tensor(6.0218, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "286 tensor(6.0379, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "287 tensor(5.6976, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "288 tensor(5.6081, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "289 tensor(5.5498, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "290 tensor(5.6500, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "291 tensor(6.3466, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "292 tensor(5.9138, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "293 tensor(5.8063, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "294 tensor(5.8903, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "295 tensor(5.7900, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "296 tensor(5.5469, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "297 tensor(5.9227, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "298 tensor(5.8891, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n",
            "299 tensor(5.9613, device='mps:0', grad_fn=<NllLoss2DBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Train the network\n",
        "# Create an optimizer\n",
        "m1.train()\n",
        "optimizer = torch.optim.AdamW(m1.parameters(), lr=5e-6)\n",
        "train_dataloader_iter = iter(train_dataloader)\n",
        "\n",
        "for epoch in range(300):\n",
        "  try:\n",
        "    batch = next(train_dataloader_iter)\n",
        "  except StopIteration:\n",
        "    # Reset the dataloader\n",
        "    train_dataloader_iter = iter(train_dataloader)\n",
        "    batch = next(train_dataloader_iter)\n",
        "  input = torch.cat((batch[0], batch[1], batch[2]), dim=1) # (B, block_size)\n",
        "  target = batch[3] # (B, max_len_target)\n",
        "  ys, loss = m1(input, target)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(epoch, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "JrjMkTStimxt"
      },
      "outputs": [],
      "source": [
        "# After training\n",
        "# Save the model\n",
        "torch.save(m1.state_dict(), \"transform1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3086, 727, 3000, 4693, 3256, 2966, 2746, 1216, 1432, 2356, 5396, 3178, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 4143, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185]\n",
            "['<START>', 'SELECT', 'StuID', 'FROM', 'Student', 'WHERE', 'city_code', '=', '``', 'CHI', \"''\", '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min']\n",
            "['CLS', 'number', 'student', 'id', 'CLS', 'text', 'last', 'name', 'CLS', 'text', 'first', 'name', 'CLS', 'number', 'age', 'CLS', 'text', 'sex', 'CLS', 'number', 'major', 'CLS', 'number', 'advisor', 'CLS', 'text', 'city', 'code', 'CLS', 'number', 'game', 'id', 'CLS', 'text', 'game', 'name', 'CLS', 'text', 'game', 'type', 'CLS', 'number', 'student', 'id', 'CLS', 'number', 'game', 'id', 'CLS', 'number', 'hour', 'play', 'CLS', 'number', 'student', 'id', 'CLS', 'text', 'sport', 'name', 'CLS', 'number', 'hour', 'per', 'week', 'CLS', 'number', 'game', 'play', 'CLS', 'text', 'on', 'scholarship', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'CLS', 'student', 'CLS', 'video', 'game', 'CLS', 'play', 'game', 'CLS', 'sport', 'info', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'What', 'are', 'the', 'ids', 'of', 'all', 'students', 'who', 'live', 'in', 'CHI', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "[3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185, 3185]\n",
            "['min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min', 'min']\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "m1_trained = Transformer1()\n",
        "m1_trained.load_state_dict(torch.load(\"transform1.pt\"))\n",
        "m1_trained = m1_trained.to(DEVICE)\n",
        "m1_trained.eval()\n",
        "\n",
        "x_batch = next(iter(train_dataloader))\n",
        "\n",
        "input = torch.cat((x_batch[0], x_batch[1], x_batch[2]), dim=1) # (B, block_size)\n",
        "target = x_batch[3].tolist()[0]\n",
        "print(target)\n",
        "print(toks_decode(target, \"target\"))\n",
        "\n",
        "y_batch = m1_trained.generate(input, max_generated_tokens=max_len_target)\n",
        "print(toks_decode(input.tolist()[0], \"source\"))\n",
        "print(y_batch)\n",
        "print(toks_decode(y_batch, \"target\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
