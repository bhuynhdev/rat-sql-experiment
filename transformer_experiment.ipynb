{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wGMyKMCDzRfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Users/huynhlbg/Downloads/rat-sql-experiment/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/Users/huynhlbg/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 9.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.0.4)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.venv/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Users/huynhlbg/Downloads/rat-sql-experiment/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "%pip install gdown torch torchtext torchdata stanza spacy --quiet\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kXJ5VJX25HYq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS device\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import torch\n",
        "# Prepare Stanford's Stanza to tokenize the columns and tables names\n",
        "# stanza.download('en')\n",
        "# stanza_nlp = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='en')\n",
        "\n",
        "# Prepare spacy to tokenize and lemmatize the table names and columns names\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\", \"parser\"])\n",
        "# Fix the \"Spacy breaks up 'id' into 'i' and 'd' issue\" https://github.com/explosion/spaCy/discussions/10570\n",
        "spacy_nlp.tokenizer.rules = { key: value for key, value in spacy_nlp.tokenizer.rules.items() if key != \"id\" }\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "  print(\"Using MPS device\")\n",
        "  device = \"mps\" # Apple Metal Performance Shader (M1 chip)\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using CUDA device\")\n",
        "  device = \"cuda\"\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VKMceBe22STs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "spider_url = 'https://drive.google.com/u/0/uc?id=1iRDVHLr4mX2wQKSgA9J8Pire73Jahh0m&export=download' # https://yale-lily.github.io/spider\n",
        "output = 'spider.zip'\n",
        "if not os.path.exists(output):\n",
        "  gdown.download(spider_url, output, quiet=False)\n",
        "\n",
        "!if [ ! -d \"spider\" ]; then unzip spider.zip -d .; else echo \"The 'spider' folder already exists.\"; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz75rWXgzBRd",
        "outputId": "280a2d12-e04a-4388-906a-aa9655690682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column(db_id='perpetrator', table_index=0, col_index=1, name='perpetrator id', name_toks=['perpetrator', 'id'], orig_name='Perpetrator_ID', col_type='number')\n",
            "Database(db_id='entrepreneur', raw_schema={'column_names': [[-1, '*'], [0, 'entrepreneur id'], [0, 'people id'], [0, 'company'], [0, 'money requested'], [0, 'investor'], [1, 'people id'], [1, 'name'], [1, 'height'], [1, 'weight'], [1, 'date of birth']], 'column_names_original': [[-1, '*'], [0, 'Entrepreneur_ID'], [0, 'People_ID'], [0, 'Company'], [0, 'Money_Requested'], [0, 'Investor'], [1, 'People_ID'], [1, 'Name'], [1, 'Height'], [1, 'Weight'], [1, 'Date_of_Birth']], 'column_types': ['text', 'number', 'number', 'text', 'number', 'text', 'number', 'text', 'number', 'number', 'text'], 'db_id': 'entrepreneur', 'foreign_keys': [[2, 6]], 'primary_keys': [1, 6], 'table_names': ['entrepreneur', 'people'], 'table_names_original': ['entrepreneur', 'people']}, tables=[Table(db_id='entrepreneur', table_index=0, name='entrepreneur', name_toks=['entrepreneur'], orig_name='entrepreneur', columns=[Column(db_id='entrepreneur', table_index=0, col_index=1, name='entrepreneur id', name_toks=['entrepreneur', 'id'], orig_name='Entrepreneur_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=2, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=3, name='company', name_toks=['company'], orig_name='Company', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=4, name='money requested', name_toks=['money', 'request'], orig_name='Money_Requested', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=5, name='investor', name_toks=['investor'], orig_name='Investor', col_type='text')]), Table(db_id='entrepreneur', table_index=1, name='people', name_toks=['people'], orig_name='people', columns=[Column(db_id='entrepreneur', table_index=1, col_index=6, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=7, name='name', name_toks=['name'], orig_name='Name', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=8, name='height', name_toks=['height'], orig_name='Height', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=9, name='weight', name_toks=['weight'], orig_name='Weight', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=10, name='date of birth', name_toks=['date', 'of', 'birth'], orig_name='Date_of_Birth', col_type='text')])], columns=[Column(db_id='entrepreneur', table_index=-1, col_index=0, name='*', name_toks=['*'], orig_name='*', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=1, name='entrepreneur id', name_toks=['entrepreneur', 'id'], orig_name='Entrepreneur_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=2, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=3, name='company', name_toks=['company'], orig_name='Company', col_type='text'), Column(db_id='entrepreneur', table_index=0, col_index=4, name='money requested', name_toks=['money', 'request'], orig_name='Money_Requested', col_type='number'), Column(db_id='entrepreneur', table_index=0, col_index=5, name='investor', name_toks=['investor'], orig_name='Investor', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=6, name='people id', name_toks=['people', 'id'], orig_name='People_ID', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=7, name='name', name_toks=['name'], orig_name='Name', col_type='text'), Column(db_id='entrepreneur', table_index=1, col_index=8, name='height', name_toks=['height'], orig_name='Height', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=9, name='weight', name_toks=['weight'], orig_name='Weight', col_type='number'), Column(db_id='entrepreneur', table_index=1, col_index=10, name='date of birth', name_toks=['date', 'of', 'birth'], orig_name='Date_of_Birth', col_type='text')])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, Any, Union\n",
        "\n",
        "# Start by loading in the databases, tables, and columns schemas\n",
        "\n",
        "with open(\"spider/tables.json\") as f:\n",
        "  raw_schema_data = json.load(f)\n",
        "\n",
        "ColumnType = Literal[\"time\", \"text\", \"number\", \"boolean\", \"others\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Column:\n",
        "  db_id: str  # Which database this col belongs to\n",
        "  table_index: int  # Which specific table in the db this col belongs to\n",
        "  col_index: int  # Which specific column in this table\n",
        "  name: str\n",
        "  name_toks: list[str]  # Tokenized name\n",
        "  orig_name: str\n",
        "  col_type: ColumnType\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Table:\n",
        "  db_id: str  # Which database this table belongs to\n",
        "  table_index: int  # Which specific table in this database\n",
        "  name: str\n",
        "  name_toks: list[str]  # Tokenized name\n",
        "  orig_name: str\n",
        "  columns: list[Column] = field(default_factory=list)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Database:\n",
        "  db_id: str\n",
        "  raw_schema: Any  # The original raw schema from json file\n",
        "  tables: list[Table] = field(default_factory=list)\n",
        "  columns: list[Column] = field(default_factory=list)\n",
        "\n",
        "\n",
        "schema_lookup: dict[str, Database] = {}  # Map the db_id to that database's schema\n",
        "\n",
        "raw_schema_data[0].keys()\n",
        "# ['column_names', 'column_names_original', 'column_types', 'db_id',\n",
        "# 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original']\n",
        "\n",
        "\n",
        "def lemmatized_tokenizer(s: str):\n",
        "  doc = spacy_nlp(s)\n",
        "  lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
        "  # doc = stanza_nlp(s)\n",
        "  # lemmatized_tokens = [word.lemma.lower() for sent in doc.sentences  for word in sent.words]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "\n",
        "for i, db_info in enumerate(raw_schema_data):\n",
        "  db_id = db_info[\"db_id\"]\n",
        "  db = Database(db_id, raw_schema=db_info)\n",
        "\n",
        "  # Iterate the tables of this database\n",
        "  for table_index, (tbl_name, tbl_orig_name) in enumerate(\n",
        "    zip(db_info[\"table_names\"], db_info[\"table_names_original\"])\n",
        "  ):\n",
        "    new_table = Table(\n",
        "      db_id,\n",
        "      table_index=table_index,\n",
        "      name=tbl_name,\n",
        "      name_toks=lemmatized_tokenizer(tbl_name),\n",
        "      orig_name=tbl_orig_name,\n",
        "    )\n",
        "    db.tables.append(new_table)\n",
        "\n",
        "  # Iterate the columns of this database\n",
        "  for col_index, ((table_idx, col_name), (_, col_orig_name), col_type) in enumerate(\n",
        "    zip(\n",
        "        db_info[\"column_names\"],\n",
        "        db_info[\"column_names_original\"],\n",
        "        db_info[\"column_types\"],\n",
        "    )\n",
        "  ):\n",
        "    new_col = Column(\n",
        "        db_id=db_id,\n",
        "        col_index=col_index,\n",
        "        col_type=col_type,\n",
        "        table_index=table_idx,\n",
        "        name=col_name,\n",
        "        name_toks=lemmatized_tokenizer(col_name),\n",
        "        orig_name=col_orig_name,\n",
        "    )\n",
        "    db.columns.append(new_col)\n",
        "    table_idx: int = table_idx\n",
        "    if table_idx >= 0:  # Skip the -1 index table\n",
        "        db.tables[table_idx].columns.append(new_col)\n",
        "\n",
        "  schema_lookup[db_id] = db\n",
        "\n",
        "all_columns = [col for s in schema_lookup.values() for col in s.columns]\n",
        "all_tables = [tbl for s in schema_lookup.values() for tbl in s.tables]\n",
        "\n",
        "print(all_columns[1])\n",
        "print(schema_lookup[\"entrepreneur\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": true,
        "id": "wv5MghOvMVqh"
      },
      "outputs": [],
      "source": [
        "# Load the questions\n",
        "with open(\"spider/train_spider.json\") as f:\n",
        "  raw_train_data = json.load(f)\n",
        "\n",
        "with open(\"spider/dev.json\") as f:\n",
        "  raw_val_data = json.load(f)\n",
        "\n",
        "raw_train_data[0].keys()\n",
        "# ['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql']\n",
        "\n",
        "@dataclass\n",
        "class QAPair: # Class to encapsulate each NL query with corresponding SQL answer\n",
        "  db_id: str\n",
        "  question_toks: list[str]\n",
        "  question: str\n",
        "  query_toks: list[str]\n",
        "  query: str\n",
        "  sql_tree: Any # Raw sql tree as a nested dict from the raw JSON\n",
        "\n",
        "@dataclass\n",
        "class SpiderItem:\n",
        "  \"\"\"An item to feed into the neural network. Basically encapsulation of a training example\"\"\"\n",
        "  db_id: str\n",
        "  qa_pair: QAPair\n",
        "  db: Database # Information about the database used in the qa\n",
        "\n",
        "\n",
        "train_qas: list[QAPair] = []\n",
        "val_qas: list[QAPair] = []\n",
        "train_items: list[SpiderItem] = []\n",
        "val_items: list[SpiderItem] = []\n",
        "\n",
        "for i, qa_info in enumerate(raw_train_data):\n",
        "  db_id = qa_info[\"db_id\"]\n",
        "  new_qa = QAPair(\n",
        "    db_id=db_id,\n",
        "    question_toks=qa_info[\"question_toks\"],\n",
        "    question=qa_info[\"question\"],\n",
        "    query_toks=qa_info[\"query_toks\"],\n",
        "    query=qa_info[\"query\"],\n",
        "    sql_tree=qa_info[\"sql\"]\n",
        "  )\n",
        "  new_item = SpiderItem(\n",
        "    db_id=db_id,\n",
        "    qa_pair=new_qa,\n",
        "    db=schema_lookup[db_id]\n",
        "  )\n",
        "  train_qas.append(new_qa)\n",
        "  train_items.append(new_item)\n",
        "\n",
        "for i, qa_info in enumerate(raw_val_data):\n",
        "  db_id = qa_info[\"db_id\"]\n",
        "  new_qa = QAPair(\n",
        "    db_id=db_id,\n",
        "    question_toks=qa_info[\"question_toks\"],\n",
        "    question=qa_info[\"question\"],\n",
        "    query_toks=qa_info[\"query_toks\"],\n",
        "    query=qa_info[\"query\"],\n",
        "    sql_tree=qa_info[\"sql\"]\n",
        "  )\n",
        "  new_item = SpiderItem(\n",
        "    db_id=db_id,\n",
        "    qa_pair=new_qa,\n",
        "    db=schema_lookup[db_id]\n",
        "  )\n",
        "  val_qas.append(new_qa)\n",
        "  val_items.append(new_item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK7Sg2WoIT2C",
        "outputId": "556adaf2-cd93-43ac-9057-d154b9da4943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'boolean', 'time', 'text', 'number', 'others'}\n",
            "Vocab size col names 1176\n",
            "Vocab size tbl names 506\n",
            "['human', 'if', 'feature', 'check', '3b', 'dir', 'model', 'alias', 'phrase', 'monthly']\n",
            "['feature', 'check', 'model', 'phrase', 'match', 'season', 'availability', 'contestant', 'characteristic', 'family']\n"
          ]
        }
      ],
      "source": [
        "col_name_vocab: set[str] = set()\n",
        "tbl_name_vocab: set[str] = set()\n",
        "col_types: set[str] = set()\n",
        "\n",
        "# Build col names and tbl names vocab\n",
        "for col in all_columns:\n",
        "  col_name_vocab.update(col.name_toks)\n",
        "  col_types.add(col.col_type)\n",
        "\n",
        "for tbl in all_tables:\n",
        "  tbl_name_vocab.update(tbl.name_toks)\n",
        "\n",
        "print(col_types) # {'text', 'others', 'number', 'time', 'boolean'}\n",
        "print(\"Vocab size col names\", len(col_name_vocab))\n",
        "print(\"Vocab size tbl names\", len(tbl_name_vocab))\n",
        "print(list(col_name_vocab)[0:10])\n",
        "print(list(tbl_name_vocab)[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFKZz5rdySzh",
        "outputId": "0c14015e-1c57-4854-f94c-5af7431a1673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size questions 3750\n",
            "Vocab size SQL 5468\n"
          ]
        }
      ],
      "source": [
        "target_vocab: set[str] = set() # Vocab of the SQL target\n",
        "question_vocab: set[str] = set() # Vocab of the Natural Language questions\n",
        "\n",
        "for qa_pair in train_qas:\n",
        "  question_vocab.update(qa_pair.question_toks)\n",
        "  target_vocab.update(qa_pair.query_toks)\n",
        "\n",
        "for qa_pair in val_qas:\n",
        "  question_vocab.update(qa_pair.question_toks)\n",
        "  target_vocab.update(qa_pair.query_toks)\n",
        "\n",
        "# Padding token used to pad shorter inputs so every sequence of inputs to the encoder\n",
        "# is of same length (which would be the max length)\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "src_vocab: set[str] = set() # Source vocab used for inputs to the encoder, which includes questions & columns & tables names\n",
        "src_vocab.add(PAD_TOKEN)\n",
        "# Add the \"CLS\" token as a separator between each part in the input block (i.e. separator between each column name, table name, etc.)\n",
        "src_vocab.add(\"CLS\")\n",
        "src_vocab.update(col_name_vocab)\n",
        "src_vocab.update(col_types)\n",
        "src_vocab.update(tbl_name_vocab)\n",
        "src_vocab.update(question_vocab)\n",
        "\n",
        "tgt_vocab: set[str] = set() # Target vocab used for decoder output; which is the set of SQL tokens plus the Start and End tokens\n",
        "tgt_vocab.add(\"<START>\")\n",
        "tgt_vocab.add(\"<END>\")\n",
        "tgt_vocab.add(PAD_TOKEN)\n",
        "tgt_vocab.update(target_vocab)\n",
        "\n",
        "\n",
        "print(\"Vocab size questions\", len(question_vocab))\n",
        "print(\"Vocab size SQL\", len(target_vocab))\n",
        "\n",
        "# Token to id: Assign a number to each token in the vocab\n",
        "# Id to token: Retrieve the token give the index\n",
        "src_tok_to_idx = { tok: i for i, tok in enumerate(src_vocab) }\n",
        "src_idx_to_tok = { i: tok for i, tok in enumerate(src_vocab) }\n",
        "tgt_tok_to_idx = { tok: i for i, tok in enumerate(tgt_vocab) }\n",
        "tgt_idx_to_tok = { i: tok for i, tok in enumerate(tgt_vocab) }\n",
        "\n",
        "def toks_encode(tokens_list: list[str], mode: Literal[\"source\", \"target\"]):\n",
        "  \"\"\"Given a list of tokens, 'encode' them to return a list of token_ids\n",
        "  Parameters:\n",
        "    mode: Whether to use the source's or target's tok_to_idx lookup table\n",
        "  \"\"\"\n",
        "  if mode == \"source\":\n",
        "    return [src_tok_to_idx[token] for token in tokens_list]\n",
        "  return [tgt_tok_to_idx[token] for token in tokens_list]\n",
        "\n",
        "def toks_decode(ids_list: list[int], mode: Literal[\"source\", \"target\"]):\n",
        "  \"\"\"Given a list of token_ids, 'decode' them to return the list of corresponding token\n",
        "  Parameters:\n",
        "    mode: Whether to use the source's or target's idx_to_tok lookup table\n",
        "  \"\"\"\n",
        "  if mode == \"source\":\n",
        "    return [src_idx_to_tok[token_id] for token_id in ids_list]\n",
        "  return [tgt_idx_to_tok[token_id] for token_id in ids_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzijLcv7E5F8",
        "outputId": "7e915550-7778-497d-f91d-0b3ff2878aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7000\n",
            "1034\n",
            "[1196, 70, 44, 91]\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset and dataloader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def construct_input_block_toks(\n",
        "    col_toks: list[list[str]],\n",
        "    col_types: list[ColumnType],\n",
        "    table_toks: list[list[str]],\n",
        "    ques_toks: list[str],\n",
        "):\n",
        "    \"\"\"Construct and return the 3 components of an input block as a tuple of the list of tokens\n",
        "    An input block has 3 parts:\n",
        "      The col name part: `CLS col_type ...col_name CLS col_type2 ...Col_name2`\n",
        "      The table name part: `CLS ...tbl_name CLS ...tbl_name2`\n",
        "      The question part: ` CLS ...question`\n",
        "\n",
        "    Parameters:\n",
        "      col_toks: nested list of all tokens across all column names, each element is a list of tokens that belongs in one column name\n",
        "      tbl_toks: nested list of all tokens across all table names, each element is a list of tokens that belongs in one table name\n",
        "\n",
        "    Returns:\n",
        "      a 3-elemment tuple containing (col_name_part, tbl_name_part, ques_part) w/ each part being the list of tokens including the 'CLS' token\n",
        "    \"\"\"\n",
        "    # List of column types should match length with list of columns\n",
        "    assert len(col_toks) == len(col_types)\n",
        "    # Construct the col name part, thus producing something like:\n",
        "    # \"CLS number builing size CLS text building name...\"\n",
        "    col_name_part: list[str] = []\n",
        "    for col_name_toks, col_type in zip(col_toks, col_types):\n",
        "        col_name_part.extend([\"CLS\", col_type] + col_name_toks)\n",
        "    # Construct the col name part, thus producing something like:\n",
        "    # \"CLS building information CLS city information ...etc\"\n",
        "    tbl_name_part: list[str] = []\n",
        "    for tbl_name_toks in table_toks:\n",
        "        tbl_name_part.extend([\"CLS\"] + tbl_name_toks)\n",
        "    # Construct the question part, thus producing something like:\n",
        "    # \"CLS what is the highest building in Chicago\"\n",
        "    question_part: list[str] = ques_toks\n",
        "    return (col_name_part, tbl_name_part, question_part)\n",
        "\n",
        "DatasetItem = tuple[list[str], list[str], list[str], list[str]]\n",
        "class MyDataset(Dataset[DatasetItem]):\n",
        "    \"\"\"Each element in this dataset is a 4-element tuple\n",
        "    (col_name_toks, table_name_toks, question_toks, target_toks)\n",
        "\n",
        "    where `col_name_toks`, `table_name_toks`, `question_toks` has been preprocessed to add \"CLS\" separator token\n",
        "    and `target_toks` is also processed to include <Start> and <End> tokens\n",
        "    \"\"\"\n",
        "    def __init__(self, all_items: list[SpiderItem]):\n",
        "        self.items = all_items\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> DatasetItem:\n",
        "        item = self.items[idx]\n",
        "        columns_tokens = [c.name_toks for c in item.db.columns[1:]] # Skip the 0th-index \"*\" column\n",
        "        columns_types: list[ColumnType] = [c.col_type for c in item.db.columns[1:]]\n",
        "        tables_tokens = [t.name_toks for t in item.db.tables]\n",
        "        question_tokens = item.qa_pair.question_toks\n",
        "        target_tokens = item.qa_pair.query_toks + [\"<END>\"]\n",
        "\n",
        "        return construct_input_block_toks(columns_tokens, columns_types, tables_tokens, question_tokens) + (target_tokens,)\n",
        "\n",
        "\n",
        "train_dataset = MyDataset(train_items)\n",
        "val_dataset = MyDataset(val_items)\n",
        "print(len(train_dataset))  # Should be 7000\n",
        "print(len(val_dataset)) # Should be 1034\n",
        "\n",
        "PAD_TOKEN_ID = src_tok_to_idx[PAD_TOKEN]\n",
        "\n",
        "# Find the max_length of each \"category\".\n",
        "# Category here correspondings to the 4 categories (col_name_toks, table_name_toks, question_toks, target_toks)\n",
        "max_lengths = [max(len(category) for category in item) for item in zip(*train_dataset)]\n",
        "print(max_lengths) # [1196, 70, 44, 92]\n",
        "max_len_col_name_part = max_lengths[0]\n",
        "max_len_tbl_name_part = max_lengths[1]\n",
        "max_len_ques_part = max_lengths[2]\n",
        "max_len_target = max_lengths[3]\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "ModelInput = tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "\n",
        "def pad_collate(batch: list[tuple[list[str], list[str], list[str], list[str]]]) -> ModelInput:\n",
        "    \"\"\"The collate_fn to pass to Pytorch Dataloader.\n",
        "    Pad each element in the dataset so they all have the same size\"\"\"\n",
        "    batched_col_name_toks, batched_tbl_name_toks, batched_ques_toks, batched_target_toks = zip(*batch)\n",
        "\n",
        "    batched_colname_tokIds = [torch.tensor(toks_encode(col_name_toks, mode=\"source\")) for col_name_toks in batched_col_name_toks]\n",
        "    # Now pad this batch so that every item of the batch is same size as a local longest item\n",
        "    # From the doc: `pad_sequence` stacks a list of Tensors along a new dimension, and pads them to equal length.\n",
        "    batched_colname_tokIds = pad_sequence(batched_colname_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    # Now pad this batch again so that every item of the batch is same size of the global longest item (max_len_col_name_part)\n",
        "    batched_colname_tokIds= torch.nn.functional.pad(\n",
        "        batched_colname_tokIds, (0, max_len_col_name_part - batched_colname_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_colname_tokIds = batched_colname_tokIds.to(DEVICE) # (B, T)\n",
        "\n",
        "    # After padding, each tensor should be dimension (B, T)\n",
        "    # With B being the batch dimension (since we use batch_first=True) and T is max_len_col_name_part\n",
        "    assert batched_colname_tokIds.shape == (BATCH_SIZE, max_len_col_name_part), f\"Expected {BATCH_SIZE, max_len_col_name_part}, got {batched_colname_tokIds.shape}\"\n",
        "\n",
        "    # Do the same process for table name toks, question toks, and target toks\n",
        "    batched_tblname_tokIds = [torch.tensor(toks_encode(tbl_name_toks, mode=\"source\")) for tbl_name_toks in batched_tbl_name_toks]\n",
        "    batched_tblname_tokIds = pad_sequence(batched_tblname_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    batched_tblname_tokIds= torch.nn.functional.pad(\n",
        "        batched_tblname_tokIds, (0, max_len_tbl_name_part - batched_tblname_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_tblname_tokIds = batched_tblname_tokIds.to(DEVICE)\n",
        "    assert batched_tblname_tokIds.shape == (BATCH_SIZE, max_len_tbl_name_part)\n",
        "\n",
        "    batched_ques_tokIds = [torch.tensor(toks_encode(ques_toks, mode=\"source\")) for ques_toks in batched_ques_toks]\n",
        "    batched_ques_tokIds = pad_sequence(batched_ques_tokIds, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    batched_ques_tokIds = torch.nn.functional.pad(\n",
        "        batched_ques_tokIds, (0, max_len_ques_part - batched_ques_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_ques_tokIds = batched_ques_tokIds.to(DEVICE)\n",
        "    assert batched_ques_tokIds.shape == (BATCH_SIZE, max_len_ques_part)\n",
        "\n",
        "    batched_tgt_tokIds = [torch.tensor(toks_encode(tgt_toks, mode=\"target\")) for tgt_toks in batched_target_toks]\n",
        "    # Note that for the target, we need use `tgt_tok_to_idx`\n",
        "    batched_tgt_tokIds = pad_sequence(batched_tgt_tokIds, batch_first=True, padding_value=tgt_tok_to_idx[PAD_TOKEN])\n",
        "    batched_tgt_tokIds = torch.nn.functional.pad(\n",
        "        batched_tgt_tokIds, (0, max_len_target - batched_tgt_tokIds.size(1)), value=PAD_TOKEN_ID)\n",
        "    batched_tgt_tokIds = batched_tgt_tokIds.to(DEVICE)\n",
        "    assert batched_tgt_tokIds.shape == (BATCH_SIZE, max_len_target)\n",
        "\n",
        "    return (batched_colname_tokIds, batched_tblname_tokIds, batched_ques_tokIds, batched_tgt_tokIds)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzuU6VkB3tuw"
      },
      "source": [
        "# Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C46jlVZY4H8A"
      },
      "source": [
        "## The Masked Attention Math trick\n",
        "\n",
        "The Triangular mask technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7PwwGDegHS2",
        "outputId": "9be7ecec-bcfc-42fa-ee7a-25933c2e0d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
            "        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n"
          ]
        }
      ],
      "source": [
        "# The Attention Mask Math trick - The Triangular mask technique\n",
        "T = 300\n",
        "\n",
        "# A triangular mask is a tensor like this: dimension (block_size, block_size)\n",
        "# tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
        "#        [1., 1., 0.,  ..., 0., 0., 0.],\n",
        "#        [1., 1., 1.,  ..., 0., 0., 0.],\n",
        "#        ...,\n",
        "#        [1., 1., 1.,  ..., 1., 0., 0.],\n",
        "#        [1., 1., 1.,  ..., 1., 1., 0.],\n",
        "#        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
        "tril_mask = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# Initial weight represents the initial *affinity* between each pair of tokens in the sequence\n",
        "# (right now we initialize them all with 0s, but they can be diff numbers coming from somewhere like the encoder for example)\n",
        "initial_weight = torch.zeros((T, T))\n",
        "\n",
        "# For the purpose of decoder calculation, we want prediction of the next step to only depend on values of previous steps\n",
        "# Therefore, we'll apply a *triangular mask* to the weight vector\n",
        "# By the structure of the triangular mask, we can see that, after masking,\n",
        "# we essentially \"disable\"/\"mask away\" future tokens since they will be multiplying with 0\n",
        "# since only positions corresponding to 1s will be retained. Positions with 0s is replaced with -inf\n",
        "weight = initial_weight.masked_fill(tril_mask == 0, float(\"-inf\"))\n",
        "# A softmax layer convert these *affinity scores* to a weight number between 0 and 1\n",
        "weight = F.softmax(weight, dim=-1)\n",
        "print(weight)\n",
        "# tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
        "#        ...,\n",
        "#        [0.0034, 0.0034, 0.0034,  ..., 0.0034, 0.0000, 0.0000],\n",
        "#        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0000],\n",
        "#        [0.0033, 0.0033, 0.0033,  ..., 0.0033, 0.0033, 0.0033]])\n",
        "# --> See how the weight vector showcase how to calculate a prediction based on the weighted sum of previous values only\n",
        "# For example:\n",
        "#   2nd value = 1.00 * 1st val\n",
        "#   3rd prediction = 0.5 * 1st val + 0.5 * 2nd val\n",
        "#   4th prediction = 0.33 * 1st value + 0.33 * 2nd val + 0.33 * 3rd val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNeqGOj4RSw"
      },
      "source": [
        "## Self-attention block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Un6Q6p044kTN"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "BLOCK_SIZE = max_len_col_name_part + max_len_tbl_name_part + max_len_ques_part\n",
        "DROP_OUT = 0.2\n",
        "\n",
        "class SingleHeadAttention(nn.Module):\n",
        "  def __init__(self, emb_size: int, head_size: int):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.head_size = head_size\n",
        "    # Each head has its own W_Q, W_K, and W_V matrixes for transform the each tok emd to its corresponding q, k, v vectors\n",
        "    self.query_matrix = nn.Linear(emb_size, head_size, bias=False)\n",
        "    self.key_matrix = nn.Linear(emb_size, head_size, bias=False)\n",
        "    self.value_matrix = nn.Linear(emb_size, head_size, bias=False)\n",
        "    # tril_mask is a static non-learned parameter, so need to use `register_buffer`\n",
        "    self.register_buffer(\"tril_mask\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "    self.dropout = nn.Dropout(DROP_OUT)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, should_mask:bool):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      x: the input embedding (after summing token_emb and position_emb) --> dimension (B, T, E)\n",
        "      should_mask: should this Attention block use masked attention (decoder should use mask, encoder shouldn't)\n",
        "    \"\"\"\n",
        "    query = self.query_matrix(x) # (B, T, D) with D = head_size\n",
        "    key = self.key_matrix(x) # (B, T, D)\n",
        "    value = self.value_matrix(x) # (B, T, D)\n",
        "    # `q @ k.T` will produce the itcanitial affinity matrix, basically how strong each query relates to each key\n",
        "    # dimension: (B, T, D) @ (B, D, T) = (B, T, T)\n",
        "    # Note The original \"Attention is all you need paper\" also scales down the affinity scores by multiplying `sqrt(head_size)`\n",
        "    affinity = (query @ key.transpose(-2, -1)) * (sqrt(self.head_size))  # tranpose(-2, -1) avoid transposing the Batch dimension\n",
        "    if should_mask:\n",
        "      affinity = affinity.masked_fill(self.tril_mask == 0, float(\"-inf\"))\n",
        "    weight = F.softmax(affinity, dim=-1) # (B, T, T)\n",
        "    weight = self.dropout(weight)\n",
        "    # The output is the embeddings where each token's embedding have been tweaked\n",
        "    # to also include information about other related tokens\n",
        "    out = weight @ value # (B, T, D)\n",
        "    return out\n",
        "  \n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, emb_size: int, num_head: int):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    # Each head size is emb_size / num_head so that at the end, when we concat all vectors from each head, we still get a vector of emb_size\n",
        "    self.heads = nn.ModuleList([SingleHeadAttention(emb_size, emb_size // num_head) for _ in range(num_head)])\n",
        "    self.dropout = nn.Dropout(DROP_OUT)\n",
        "  \n",
        "  def forward(self, x: torch.Tensor, should_mask: bool=False):\n",
        "    out = torch.cat([sa(x, should_mask=should_mask) for sa in self.heads], dim=-1)\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "  \n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "  \"\"\"After self-attention block is a Feed forward neural net (section 3.3)\"\"\"\n",
        "  def __init__(self, emb_size: int):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "      nn.Linear(emb_size, 4 * emb_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * emb_size, emb_size),\n",
        "      nn.Dropout(DROP_OUT)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.feed_forward(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  \"\"\"A Transformer Encoder block: A self-attention followed by feedforward net\"\"\"\n",
        "  def __init__(self, emb_size: int, num_attention_heads: int):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHeadAttention(emb_size, num_attention_heads)\n",
        "    self.feed_forward = PositionWiseFeedForward(emb_size)\n",
        "    self.layer_norm1 = nn.LayerNorm(emb_size) # Layer norm for the self-attention sublayer\n",
        "    self.layer_norm2 = nn.LayerNorm(emb_size) # Layer norm for the feed-forward sublayer\n",
        "  \n",
        "  def forward(self, x: torch.Tensor):\n",
        "    # Addition is needed to represent additive residual connection\n",
        "    x = x + self.self_attention(self.layer_norm1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JumzVuyT4X-f"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "quKt89aqF6zC"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "class Transformer1(nn.Module):\n",
        "  def __init__(self, emb_size: int = 512):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      emb_size: the size of each word embeddings. For example: GloVe embeddings is 300, BERT is 768\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    # 4 encoder blocks\n",
        "    self.encoder_layers = nn.Sequential(\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      EncoderBlock(emb_size, num_attention_heads=4),\n",
        "      nn.LayerNorm(emb_size)\n",
        "    )\n",
        "    # ENCODER COMPONENTS\n",
        "    self.encoder_token_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "    # Position embedding table: Convert each token's position in its block to a position embedding\n",
        "    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, emb_size)\n",
        "\n",
        "    # DECODER COMPONENTS\n",
        "    self.decoder_hidden_state_size = 2 * emb_size\n",
        "    # Embedding lookup table: Convert token_ids to that token's corresponding embeddings\n",
        "    self.decoder_token_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
        "    # Target language modeling head: Transform back from the embedding dimension to the tgt_vocab_size dimension\n",
        "    # So that we can get the distribution and know which target token to choose\n",
        "    self.tgt_lm_head = nn.Linear(self.decoder_hidden_state_size, tgt_vocab_size)\n",
        "    # For the decoder, we try to replicate the RNN model to process sequences\n",
        "    # decoder_hidden_state = sigmoid(W1 * context_matrix + W2 * prev_hidden_state + bias + W3 * decoder_input_tok_emb)\n",
        "    # Thus we need 3 weights matrix for use in the decoder, to produce a new decoder_hidden_state\n",
        "    self.decoder_context_linear = nn.Linear(emb_size, self.decoder_hidden_state_size)\n",
        "    self.decoder_hiddenstate_linear = nn.Linear(self.decoder_hidden_state_size, self.decoder_hidden_state_size)\n",
        "    self.decoder_token_emb_linear = nn.Linear(emb_size, self.decoder_hidden_state_size)\n",
        "\n",
        "  def forward(self, input_idx: torch.Tensor, target_idx: Union[torch.Tensor, None] = None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      target_idx: the list of target tokens across the batches. Dimension (B, T)\n",
        "    \"\"\"\n",
        "    x = self.encode(input_idx) # (B, T, E)\n",
        "    # Average the last hidden state of the encoder as the context\n",
        "    context_emb = torch.mean(x, dim=1) # (B, E)\n",
        "    # Feed the <START> token as the chosen token to the entire batch\n",
        "    chosen_tokens = torch.tensor(toks_encode([\"<START>\"] * x.size(0), \"target\"), device=DEVICE) # (B)\n",
        "    ys: list[torch.Tensor] = []\n",
        "    # Initialize the first hidden state as 0s\n",
        "    hidden_state = torch.zeros((BATCH_SIZE, self.decoder_hidden_state_size), device=DEVICE) # (B, H) where H = dec_hidden_state\n",
        "    for _ in range(max_len_target):\n",
        "      # hidden_state: (B, H)\n",
        "      # tgt_probs: (B, C) where C = tgt_vocab_size\n",
        "      hidden_state, tgt_probs = self.decode(context_emb, chosen_tokens, hidden_state)\n",
        "      # Greedily select the token with highest prob from the distribution\n",
        "      chosen_tokens = torch.argmax(tgt_probs, dim=1) # (B)\n",
        "      ys.append(tgt_probs)\n",
        "\n",
        "    # Note that ys is collected by looping over max_len_target, so when stacked, the first dimension is max_len_target\n",
        "    y = torch.stack(ys) # (T, B, C) where C = tgt_vocab_size\n",
        "    assert y.shape == (max_len_target, BATCH_SIZE, tgt_vocab_size)\n",
        "    if target_idx is None:\n",
        "      return ys, None\n",
        "    # Cross_entropy requires the \"Class\" dimension to be the 2nd dimension\n",
        "    T, B, C = y.shape\n",
        "    y = y.view(B*T, C)\n",
        "    target_idx = target_idx.view(B*T)\n",
        "    # Calculate loss\n",
        "    loss = F.cross_entropy(y, target_idx, ignore_index=toks_encode([PAD_TOKEN], \"target\")[0])\n",
        "    return ys, loss\n",
        "\n",
        "  def encode(self, input_batch: torch.Tensor):\n",
        "    # Input batch is of shape (B, T) (i.e. (batch size, block_size))\n",
        "    token_emb = self.encoder_token_emb(input_batch) # (B, T, E) where E=emb_size\n",
        "    B, T, E = token_emb.shape\n",
        "    # the position_embedding_table takes input the position of each token in the sequence (i.e. the T dimension)\n",
        "    position_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # (T, E)\n",
        "    # For the bigram model, let's not do anything with these embeddings, and just return them as logits\n",
        "    x = token_emb + position_emb # (B, T, E)\n",
        "    # Feed this x through layers of Transformer Self-Attention blocks\n",
        "    x = self.encoder_layers(x)\n",
        "    return x\n",
        "\n",
        "  def decode(self, context_emb: torch.Tensor, input_tokenIds: torch.Tensor, prev_hidden_state: torch.Tensor):\n",
        "    \"\"\"Decode the logis from the encoder to produce a target token\"\"\"\n",
        "    # Right now let's use an RNN-like decoder\n",
        "    assert context_emb.shape == (BATCH_SIZE, self.emb_size)\n",
        "    assert input_tokenIds.shape == (BATCH_SIZE,)\n",
        "    assert prev_hidden_state.shape == (BATCH_SIZE, self.decoder_hidden_state_size), f\"Got {prev_hidden_state.shape}\"\n",
        "    # For the decoder, we try to replicate the RNN model to process sequences\n",
        "    # decoder_hidden_state = tanh(W1 * context_matrix + W2 * prev_hidden_state + W3 * decoder_input_tok_emb + bias)\n",
        "    temp1 = self.decoder_context_linear(context_emb) # (B, dec_hidden_size)\n",
        "    temp2 = self.decoder_hiddenstate_linear(prev_hidden_state) # (B, dec_hiden_size)\n",
        "    tok_emb = self.decoder_token_emb(input_tokenIds) # (B, E)\n",
        "    temp3 = self.decoder_token_emb_linear(tok_emb) # (B, dec_hidden_size)\n",
        "    z = temp1 + temp2 + temp3 # (B, dec_hidden_size)\n",
        "    hidden_state = torch.tanh(z) # (B, dec_hidden_size)\n",
        "    assert hidden_state.shape == (BATCH_SIZE, self.decoder_hidden_state_size)\n",
        "\n",
        "    tgt_distribution: torch.Tensor = self.tgt_lm_head(hidden_state) # (B, tgt_vocab_size)\n",
        "    # Do NOT run softmax here as the Pytorch Cross Entropy Loss function expects unnormalized numbers\n",
        "    # tgt_probs = F.softmax(tgt_distribution, dim=-1) # (B, tgt_vocab_size)\n",
        "    return hidden_state, tgt_distribution\n",
        "\n",
        "  def generate(self, input_idx: torch.Tensor, max_generated_tokens: int = 20):\n",
        "    with torch.no_grad():\n",
        "      encoder_last_hidden_state = self.encode(input_idx) # (B, T, E)\n",
        "      # Average all input tokens embs across the encoder last hidden state as the context\n",
        "      context_emb = torch.mean(encoder_last_hidden_state, dim=1) # (B, E)\n",
        "      chosen_tokens = torch.tensor(toks_encode([\"<START>\" for _ in range(BATCH_SIZE)], \"target\"), device=DEVICE) # (B)\n",
        "      first_batch_predicted_tokens: list[int] = []\n",
        "      # Initialize the first hidden state as 0s\n",
        "      hidden_state = torch.zeros((BATCH_SIZE, self.decoder_hidden_state_size), device=DEVICE) # (B, H) where H = dec_hidden_state\n",
        "      for _ in range(max_generated_tokens):\n",
        "        # hidden_state: dimension (B, H)\n",
        "        # tgt_probs: dimension (B, tgt_vocab_size)\n",
        "        hidden_state, tgt_probs = self.decode(context_emb, chosen_tokens, prev_hidden_state=hidden_state)\n",
        "        # Greedily select the token with highest prob from the distribution\n",
        "        chosen_tokens = torch.argmax(tgt_probs, dim=1) # (B)\n",
        "        # print(chosen_tokens)\n",
        "        chosen_token = chosen_tokens[0].item() # View result of only the first batch\n",
        "        first_batch_predicted_tokens.append(int(chosen_token))\n",
        "    return first_batch_predicted_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "ePsttWMwVZi-",
        "outputId": "6482c9ea-9b66-403f-e64c-cfde5cb03dc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Open_Date',\n",
              " 'T2.winner_rank_points',\n",
              " 'hot',\n",
              " 'T4.product_id',\n",
              " 'Ref_Property_Types',\n",
              " 'T2.FacID',\n",
              " 'T1.College_ID',\n",
              " 'Participant_Details',\n",
              " 'Europe',\n",
              " 'T2.asessment_outcome_code',\n",
              " 'Pass',\n",
              " '2005',\n",
              " 'pets',\n",
              " 'Device_ID',\n",
              " 'T2.genre_id',\n",
              " 'dept_name',\n",
              " 'T2.registration_id',\n",
              " 'GORDON',\n",
              " 'Lacey',\n",
              " 'T1.Marketing_Region_Name']"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m1 = Transformer1()\n",
        "m1 = m1.to(DEVICE)\n",
        "\n",
        "m1.eval()\n",
        "x_batch = next(iter(train_dataloader))\n",
        "\n",
        "input = torch.cat((x_batch[0], x_batch[1], x_batch[2]), dim=1) # (B, block_size)\n",
        "\n",
        "first_batch_predicted_tokens = m1.generate(input)\n",
        "toks_decode(first_batch_predicted_tokens, \"target\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-3_lbn8TImMl"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "MPS backend out of memory (MPS allocated: 17.21 GB, other allocations: 830.17 MB, max allowed: 18.13 GB). Tried to allocate 418.97 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((batch[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;241m1\u001b[39m], batch[\u001b[38;5;241m2\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, block_size)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;66;03m# (B, max_len_target)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m ys, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[52], line 46\u001b[0m, in \u001b[0;36mTransformer1.forward\u001b[0;34m(self, input_idx, target_idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_idx: torch\u001b[38;5;241m.\u001b[39mTensor, target_idx: Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m  Parameters:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    target_idx: the list of target tokens across the batches. Dimension (B, T)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, E)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;66;03m# Average the last hidden state of the encoder as the context\u001b[39;00m\n\u001b[1;32m     48\u001b[0m   context_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, E)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[52], line 84\u001b[0m, in \u001b[0;36mTransformer1.encode\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m token_emb \u001b[38;5;241m+\u001b[39m position_emb \u001b[38;5;66;03m# (B, T, E)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Feed this x through layers of Transformer Self-Attention blocks\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[49], line 12\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# Addition is needed to represent additive residual connection\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m   x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m   x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(x))\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[48], line 50\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, should_mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, should_mask: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 50\u001b[0m   out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sa(x, should_mask\u001b[38;5;241m=\u001b[39mshould_mask) \u001b[38;5;28;01mfor\u001b[39;00m sa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "Cell \u001b[0;32mIn[48], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, should_mask: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 50\u001b[0m   out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[48], line 35\u001b[0m, in \u001b[0;36mSingleHeadAttention.forward\u001b[0;34m(self, x, should_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m   affinity \u001b[38;5;241m=\u001b[39m affinity\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     34\u001b[0m weight \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(affinity, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# The output is the embeddings where each token's embedding have been tweaked\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# to also include information about other related tokens\u001b[39;00m\n\u001b[1;32m     38\u001b[0m out \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m@\u001b[39m value \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/rat-sql-experiment/.venv/lib/python3.9/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.21 GB, other allocations: 830.17 MB, max allowed: 18.13 GB). Tried to allocate 418.97 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
          ]
        }
      ],
      "source": [
        "# %%script echo skipping\n",
        "# Train the network\n",
        "# Create an optimizer\n",
        "m1.train()\n",
        "optimizer = torch.optim.AdamW(m1.parameters(), lr=5e-5)\n",
        "train_dataloader_iter = iter(train_dataloader)\n",
        "\n",
        "for epoch in range(30):\n",
        "  try:\n",
        "    batch = next(train_dataloader_iter)\n",
        "  except StopIteration:\n",
        "    # Reset the dataloader\n",
        "    train_dataloader_iter = iter(train_dataloader)\n",
        "    batch = next(train_dataloader_iter)\n",
        "  input = torch.cat((batch[0], batch[1], batch[2]), dim=1) # (B, block_size)\n",
        "  target = batch[3] # (B, max_len_target)\n",
        "  ys, loss = m1(input, target)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(epoch, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrjMkTStimxt"
      },
      "outputs": [],
      "source": [
        "# %%script echo skipping\n",
        "# After training\n",
        "# Save the model\n",
        "torch.save(m1.state_dict(), \"transform1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2802, 2922, 4854, 1897, 1731, 4194, 890, 3060, 4976, 3337, 1684, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 804, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601]\n",
            "['SELECT', 'location_code', 'FROM', 'Ref_locations', 'WHERE', 'location_name', '=', '``', 'Canada', \"''\", '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels']\n",
            "['CLS', 'text', 'document', 'type', 'code', 'CLS', 'text', 'document', 'type', 'name', 'CLS', 'text', 'document', 'type', 'description', 'CLS', 'time', 'calendar', 'date', 'CLS', 'number', 'day', 'number', 'CLS', 'text', 'location', 'code', 'CLS', 'text', 'location', 'name', 'CLS', 'text', 'location', 'description', 'CLS', 'text', 'role', 'code', 'CLS', 'text', 'role', 'name', 'CLS', 'text', 'role', 'description', 'CLS', 'number', 'document', 'id', 'CLS', 'time', 'date', 'store', 'CLS', 'text', 'document', 'type', 'code', 'CLS', 'text', 'document', 'name', 'CLS', 'text', 'document', 'description', 'CLS', 'text', 'other', 'detail', 'CLS', 'number', 'employee', 'id', 'CLS', 'text', 'role', 'code', 'CLS', 'text', 'employee', 'name', 'CLS', 'text', 'gender', 'mfu', 'CLS', 'time', 'date', 'of', 'birth', 'CLS', 'text', 'other', 'detail', 'CLS', 'number', 'document', 'id', 'CLS', 'text', 'location', 'code', 'CLS', 'time', 'date', 'in', 'location', 'from', 'CLS', 'time', 'date', 'in', 'locaton', 'to', 'CLS', 'number', 'document', 'id', 'CLS', 'number', 'destruction', 'authorise', 'by', 'employee', 'id', 'CLS', 'number', 'destroy', 'by', 'employee', 'id', 'CLS', 'time', 'plan', 'destruction', 'date', 'CLS', 'time', 'actual', 'destruction', 'date', 'CLS', 'text', 'other', 'detail', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'CLS', 'reference', 'document', 'type', 'CLS', 'reference', 'calendar', 'CLS', 'reference', 'location', 'CLS', 'role', 'CLS', 'all', 'document', 'CLS', 'employee', 'CLS', 'document', 'location', 'CLS', 'document', 'to', 'be', 'destroy', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'Show', 'the', 'location', 'code', 'of', 'the', 'country', '``', 'Canada', \"''\", '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "[601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601]\n",
            "91\n",
            "['Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels', 'Huels']\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "m1_trained = Transformer1()\n",
        "m1_trained.load_state_dict(torch.load(\"transform1.pt\"))\n",
        "m1_trained = m1_trained.to(DEVICE)\n",
        "m1_trained.eval()\n",
        "\n",
        "x_batch = next(iter(train_dataloader))\n",
        "\n",
        "input = torch.cat((x_batch[0], x_batch[1], x_batch[2]), dim=1) # (B, block_size)\n",
        "target = x_batch[3].tolist()[0]\n",
        "print(target)\n",
        "print(toks_decode(target, \"target\"))\n",
        "\n",
        "y_batch = m1_trained.generate(input, max_generated_tokens=max_len_target)\n",
        "print(toks_decode(input.tolist()[0], \"source\"))\n",
        "print(y_batch)\n",
        "print(len(y_batch))\n",
        "print(toks_decode(y_batch, \"target\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
